\documentclass{article}
\usepackage{LectureNotes}
\usepackage{mathtools}

\setstretch{1.2}

\begin{comment}
\geometry{
    textheight=9in,
    textwidth=5.5in,
    top=1in,
    headheight=12pt,
    headsep=25pt,
    footskip=30pt
}
\end{comment}


\geometry
{
    a4paper,
    total={170mm,257mm},
    left=20mm,
    top=20mm,
}


% ------------------------------------------------------------------------------

\begin{document}

% ------------------------------------------------------------------------------
% Cover Page and ToC
% ------------------------------------------------------------------------------

\title{ \normalsize \textsc{}
		\\ [2.0cm]
		\HRule{1.5pt} \\
		\LARGE \textbf{\uppercase{40.017 Probability \& Statistics}
		\HRule{2.0pt} \\ [0.6cm] \LARGE{Lecture Notes} \vspace*{10\baselineskip}}
		}
\date{\today}
\author{\textbf{Michael Hoon}}

\maketitle
\newpage

\tableofcontents
\newpage

% ------------------------------------------------------------------------------


\section{Set Theory}

\subsection{Sample Spaces}

The mathematical framework for probability is built around \textit{sets}. The \textit{sample space} $S$ of an experiment is the set of all possible outcomes of the experiment. An \textit{event} $A$ is a subset of $S$, and we say that $A$ occurred if the actual outcome is in $A$.  

\subsection{Naive Definition of Probability}

Let $A$ be an event for an experiment with a finite sample space $S$. A naive probability of $A$ is 

\begin{equation}
    \mathbb{P}_{\text{naive}}(A) = \frac{|A|}{|S|} = \frac{\text{number of outcomes favorable to A}}{\text{total number of outcomes}}
\end{equation}

\noindent In general, the result about complements always holds:

\begin{equation*}
    \mathbb{P}_{\text{naive}}(A^{c}) = \frac{|A^{c}|}{|S|} = \frac{|S| - |A|}{|S|} = 1- \frac{|A|}{|S|} = 1 - \mathbb{P}_{\text{naive}}(A)
\end{equation*}

An important factor about the naive definition is that it is restrictive in requiring $S$ to be finite. 

\subsection{General Definition of Probability}

\begin{definition}
    A probability space consists of a sample space $S$ and a probability function $P$ which takes an event $A \subseteq S$ as input and returns $P(A)$, where $P(A) \in \mathbb{R}, \; P(A) \in [0, 1]$. The function must satisfy the following axioms:

    \begin{enumerate}
        \item $\mathbb{P}(\emptyset) = 1, \; \mathbb{P}(S) = 1$ 
        \item $\mathbb{P}(A) \geq 0$
        \item If $A_{1}, A_{2}, \dots$ are \textbf{disjoint events}, then: \begin{equation*}
            \mathbb{P} \left( \bigcup_{j=1}^{\infty} \right) = \sum_{j=1}^{\infty} \mathbb{P}(A_j)
        \end{equation*}
        \noindent Disjoint events are \textbf{mutually exclusive} (i.e. $A_i \cap A_j = \emptyset \; \forall \; i \neq j$).
    \end{enumerate}
\end{definition}

\subsubsection{Properties of Probability}
\begin{theorem}

Probability has the following properties, for any events $A$ and $B$: 

\begin{enumerate}
    \item $\mathbb{P}(A^{c}) = 1 - \mathbb{P}(A)$
    \item If $A \subseteq B$, then $\mathbb{P}(A) \leq \mathbb{P}(B)$
    \item $\mathbb{P}(A \cup B) = \mathbb{P}(A) + \mathbb{P} (B) - \mathbb{P}(A \cap B)$ 
\end{enumerate}
\end{theorem}

\subsubsection{Inclusion-Exclusion Principle}

For any events $A_{1}, \dots A_n$, 

\begin{equation}
    \mathbb{P}\left( \bigcup_{i=1}^{n} A_i \right) = \sum_{i} \mathbb{P}(A_i) - \sum_{i<j} \mathbb{P}(A_i \cap A_j) + \sum_{i<j<k} \mathbb{P}(A_i \cap A_j \cap A_k) - \dots + (-1)^{n+1} \mathbb{P}(A_{1} \cap \dots \cap A_n)
\end{equation}

\noindent For $n=2$, we have a nicer result:

\begin{equation*}
    \mathbb{P}(A_{1} \cup A_{2}) = \mathbb{P}(A_{1}) + \mathbb{P}(A_{2}) - \mathbb{P}(A_{1} \cap A_{2})
\end{equation*}

\subsection{Conditional Probability}

\begin{definition}
    If $A$ and $B$ are events with $\mathbb{P}(B) > 0$, then the \textit{conditional probability} of $A$ given $B$, denoted by $\mathbb{P}(A \mid B)$ is defined as: 

    \begin{equation*}
        \mathbb{P}(A | B) = \frac{\mathbb{P}(A \cap B)}{\mathbb{P}(B)}
    \end{equation*}
\end{definition}

\noindent Here $A$ is the event whose uncertainty we want to update, and $B$ is the evidence we observe. $\mathbb{P}(A)$ is the \textit{prior} probability of $A$ and $\mathbb{P}(A | B)$ is the \textit{posterior} probability of $A$. (For any event $A$, $\mathbb{P}(A|A) = \frac{\mathbb{P}(A \cap A)}{\mathbb{P}(A)}$).




\section{Derangement}

A derangement is a permutation of the elements of a set in which no element appears in its original position. We use $D_n$ to denote the number of derangements of $n$ distinct objects. 

\subsection{Counting Derangements}
We consider the number of ways in which $n$ hats ($h_{1}, \dots, h_n$) can be returned to $n$ people ($P_{1}, \dots, P_n$) such that no hat makes it back to its owner. \\

We obtain the recursive formula:

\begin{equation}\label{1-eq: derangement}
    D_n = (n-1)(D_{n-1} + D_{n-2}), \; \forall \; n \geq 2
\end{equation}

\noindent With the initial conditions $D_{1} = 0$ and $D_{2} = 1$, we can use the formula to recursively compute $D_n$ for any $n$. \\

\noindent There are various other expressions for $D_n$, equivalent to formula \ref{1-eq: derangement}:

\begin{equation}\label{1-eq: derangement sum}
    D_n = n! \sum_{i=0}^{n} \frac{(-1)^{i}}{i!}, \; \forall \; n \geq 0
\end{equation}

\subsubsection{Limiting Growth}
From Equation \ref{1-eq: derangement sum}, and the taylor series expansion for $e$:

\begin{equation}
    e^{x} = \sum_{i=0}^{\infty} \frac{x^{i}}{i!} 
\end{equation}

\noindent we substitute $x=-1$ and obtain the limiting value as $n \to \infty$:

\begin{equation*}
    \lim_{n \to \infty} \frac{D_n}{n!} = \lim_{n \to \infty} \sum_{i=0}^{n} \frac{(-1)^{i}}{i!} = e^{-1} \approx 0.367879\dots
\end{equation*}

\noindent This is the limit of the probability that a randomly selected permutation of a large number of objects is a derangement. The probability converges to this limit extremely quickly as $n$ increases, which is why $D_n$ is the nearest integer to $\frac{n!}{e}$. 

\section{Discrete Random Variables}

We formally define a random variable:

\begin{definition}
    Given an experiment with sample space $S$, a \textit{random variable} (r.v.) is a function from the sample space $S$ to the real numbers $\mathbb{R}$. It is common to denote random variables by capital letters. 
\end{definition}

\noindent Thus, a random variable $X$ assigns a numerical value $X(s)$ to each possible outcome $s$ of the experiment. The randomness comes from the fact that we have a random experiment (with Probabilities described by the probability function $P$); the mapping itself is deterministic. \\

There are two main types of random variables used in practice: \textit{discrete} and \textit{continuous} r.v.s. 

\begin{definition}
    A random variable $X$ is said to be \textit{discrete} if there is a finite list of values $a_{1}, a_{2}, \dots, a_n$ or an infinite list of values $a_{1}, a_{2}, \dots$ such that $\mathbb{P}(X = a_j \; \text{for some }j) = 1$. If $X$ is a discrete r.v., then the finite or countably infinite set of values $x$ such that $P(X = x) > 0$ is called the \textit{support} of $X$.  
\end{definition}







\subsection{Binomial}

\subsection{Hypergeometric}

If we have an urn filled with $w$ white and $b$ black balls, then drawing $n$ balls out of the urn \textit{with replacement} yields a $\text{Binom}(n, \frac{w}{(w+b)})$. If we instead sample \textit{without replacement}, then the number of white balls follow a \textbf{Hypergeometric} distribution. 

\begin{theorem}
    If $X \sim \text{hypgeo}(n, j, k)$, then the PMF of $X$ is:

    \begin{equation*}
        \mathbb{P}(X = x) = \frac{\binom{j}{x}\binom{k}{n-x}}{\binom{j+k}{n}}
    \end{equation*}
    \noindent $\forall x \in \mathbb{Z}$ satisfying $0\leq x \leq n$ and $0\leq n-x \leq j$, and $P(X=x) = 0$ otherwise. 
\end{theorem}

\noindent If $j$ and $k$ are large compared to $n$, then selection without replacement can be approximated by selection with replacement. In that case, the hypergeometric RV $X \sim \text{hypgeo}(n,j,k)$ can be approximated by a binomial RV $Y \sim \text{binomial}(n,p)$, where $p := \frac{j}{j+k}$ is the probability of selecting a black marble. \\

\noindent We can also write $X$ as the sum of (dependent) Bernoulli random variables:

\begin{equation*}
    X = X_{1} + X_{2} + \dots + X_n
\end{equation*}

where each $X_i$ equals 1 if the $i$th selected marble is black, and 0 otherwise. 


\subsubsection{Hypergeometric Symmetry}

\begin{theorem}
    The hypergeo$(w,b,n)$ and hypergeo$(n,w+b-n,w)$ distributions are identical. 
\end{theorem}

\noindent The proof follows from swapping the two sets of tags in the Hypergeometric story (white/black balls in urn) \footnote[3]{The binomial and hypergeometric distributions are often confused. Note that in Binomial distributions, the Bernoulli trials are \textbf{independent}. The Bernoulli trials in Hypergeometric distribution are \textbf{dependent}, since the sampling is done \textit{without replacement}.}. 


\subsection{Geometric}

\subsection{Negative Binomial}

In a sequence of independent Bernoulli trials with success probability $p$, if $X$ is the number of failures before the $r$th success, then $X$ is said to have the Negative Binomial distribution with parameters $r$ and $p$, denoted $X \sim \text{NBin}(r,p)$. \\ 

\noindent Both the Binomial and Negative Binomial distributions are based on independent Bernoulli trials; they differ in the \textit{stopping rule} and in what they are counting. The Negative Binomial counts the \textbf{number of failures until a fixed number of successes}. 

\begin{theorem}
    If $X \sim \text{NBin}(r,p)$, then the PMF of $X$ is 

    \begin{equation}
        P(X=x) = \binom{x-1}{n-1} (1-p)^{x-n}p^{n}, \; \forall \; x\geq n 
    \end{equation}
\end{theorem}





\section{Law of Large numbers}
Assume that we have i.i.d. $X_{1}, X_{2}, \dots$ with finite mean $\mu$ and finite variance $\sigma^{2}$. Let $\bar{X}_n = \frac{1}{n} \sum_{i=1}^{n} X_i$.

\begin{definition}
    The (Weak) Law of Large Numbers (LLN) says that as $n$ grows, the sample mean $\bar{X}_n$ converges to the true mean $\mu$. Mathematically, 
    \begin{equation}
        \forall \epsilon > 0, \; \mathbb{P}(|\bar{X}_n - \mu < \epsilon) = 1, \; \text{as} \; n \to \infty 
    \end{equation}

    \noindent For any positive margin $\epsilon$, as $n$ gets arbitrarily large, the probability that $\bar{X}_n$ is within $\epsilon$ of $\mu$ approaches 1. 
\end{definition}

\noindent Note that the LLN does not contradict the fact that a coin is memoryless (in the repeated coin toss experiment). The LLN states that the proportion of Heads converges to $\frac{1}{2}$, but this does not imply that after a long string of Heads, the coin is "due" for a Tails to "\textit{balance things out}". Rather, the convergence takes place through \textit{swamping}: past tosses are swamped by the infinitely many tosses that are yet to come.  

\subsection{Inequalities}
The inequalities in this section provide bounds on the probability of an r.v. taking on an 'extreme' value in the right or left rail of a distribution. 

\subsubsection{Markov's Inequality}
\begin{definition}
    Let $X$ be any random variable that takes only non-negative values, that is, $\mathbb{P}(X < 0) = 0$. Then for any constant $a >0$, we have: 
    \begin{equation}
        \mathbb{P}(X \geq a) \leq \frac{\mathbb{E}(X)}{a}
    \end{equation}
\end{definition}

\noindent For an intuitive interpretation, let $X$ be the income of a randomly selected individual from a population. Taking $a = \mathbb{E}(X)$, Markov's Inequality says that $\mathbb{P(X \geq 2 \mathbb{E}(X)) \leq \frac{1}{2}}$. i.e., it is impossible for more than half the population to make at least twice the average income. 

\subsubsection{Chebyshev's Inequality}
Gives general bounds for the probability of being $k$ standard deviations (SD) away from the mean. 

\begin{definition}
    Let $Y$ be any random variable with mean $\mu < \infty$ and variance $\sigma^{2} > 0$. Then for any constant $k>0$, we have: 
    \begin{equation}
        \mathbb{P}(|Y-\mu| \geq k\sigma) \leq \frac{1}{k^{2}}
    \end{equation} 
\end{definition}

\section{Central Limit Theorem}
Let $X_{1}, X_{2}, \dots$ be i.i.d. with mean $\mu$ and variance $\sigma^{2}$. 

\begin{definition}
    The CLT states that for large $n$, the distribution of $\bar{X}_n$ after standardisation approaches a standard Normal distribution. By standardisation, we mean that we subtract $\mu$, the mean of $\bar{X}_n$, and divide by $\frac{\sigma}{\sqrt{n}}$, the standard deviation of $\bar{X}_n$. 
    \begin{equation}
        \lim_{n \to \infty} \mathbb{P}(\frac{\bar{X}_n - \mu}{\frac{\sigma}{\sqrt{n}}} \leq x) = \Phi(x)    
    \end{equation}
    \noindent which is the cdf of the standard normal. Informally, when $n$ is large ($\geq 30$), then $\bar{X}_n$ and $\sum_{i=1}^{n} X_i$ can each be approximated by a normal RV with the same mean and variance; the actual distribution of $X_i$ becomes irrelevant:
    \begin{equation*}
        \bar{X}_n \approx N(\mu, \frac{\sigma^{2}}{n}), \qquad \sum_{i=1}^{n} X_i \approx N(n\mu, n\sigma^{2})
    \end{equation*}
\end{definition}

\section{Moments}

\subsection{Interpreting Moments}
\begin{definition}
    Let $X$ be an r.v. with mean $\mu$ and variance $\sigma^{2}$. For any positive integer $n$, the $n^{\text{th}}$ moment of $X$ is $\mathbb{E}(X^{n})$, the $n^{\text{th}}$ central moment is $\mathbb{E}((X - \mu)^{n})$. 
\end{definition}

\noindent In particular, the mean is the first moment and the variance is the second central moment. 

\subsection{Moment Generating Functions}
A moment generating function, as its name suggests, is a generating function that encodes the \textbf{moments} of a distribution. Starting with an infinite sequence ($a_{0}, a_{1}, a_{2}, \dots$), we 'condense' or 'store' it as a single function $g$, the generating function of the sequence:

\begin{equation*}
    \sum_{n=0}^{\infty} a_n \frac{t^{n}}{n!} \coloneqq g(t) 
\end{equation*}

\begin{definition}
    When we take $a_n = \mathbb{E}(X^{n})$, the resulting generating function is known as the \textbf{moment generating function (MGF)} of $X$, and is denoted by $M_X(t)$. \\ 

    \noindent The MGF of $X$ can be computed as an expected value: 

    % \begin{equation}
    %     M_X (t) = \sum_{n=0}^{\infty} \mathbb{E(X^{n}) \frac{t^{n}}{n!} = \sum_{n=0}^{\infty} \mathbb{E}\left(\frac{t^{n}}{n!}\right) \\
    %     = \mathbb{E}\left(\sum_{n=1}^{\infty}\frac{(tX)^{n}}{n!}\right) \\
    %     = \mathbb{E}(e^{tX})
    % \end{equation}
\end{definition}

\noindent Note that $M_X (0) = 1$ for any valid MGF. 

\subsection{Formulas \& Theorems}
Some important formulas for the MGF of $X$:

\begin{equation}
    \boxed{M_X (t) = \mathbb{E} (e^{tX})} 
\end{equation}

\noindent where if $X$ is \textbf{discrete} with pmf $f$, then 

\begin{equation}
    M_X (t) = \sum_{all x_i} e^{t x_i} f(x_i) 
\end{equation}

\noindent and if $X$ is \textbf{continuous} with pdf $f$, then 

\begin{equation}
    M_X (t) = \int_{-\infty}^{\infty} e^{tx} f(x) \, \mathrm{d}x
\end{equation}

\begin{theorem}
    Given the MGF of $X$, we can get the $n^{\text{th}}$ moment of $X$ by evaluating the $n^{\text{th}}$ derivative of the MGF at 0: 

    \begin{equation}
        \boxed{\mathbb{E}(X^{n}) = M_X^{(N)} (0)} 
    \end{equation}
\end{theorem}

\begin{theorem}
    If $X$ and $Y$ are independent, then the MGF of $X + Y$ is the product of the individual MGFs:
    \begin{equation}
        M_{X+Y} (t) = M_X(t)M_Y(t)
    \end{equation}
    \noindent This is true because if $X$ and $Y$ are independent, then $\mathbb{E}(e^{t(X+Y)}) = \mathbb{E}(e^{tX})\mathbb{E}(e^{tY})$ 
\end{theorem}

\begin{theorem}
    If two random variables have the same MGF, then they have the same distribution (same cdf, equivalently, same pdf or pmf) \footnote{For this to apply, the MGF needs to exist in an open interval around $t=0$}.
\end{theorem}

\subsection{Examples (Discrete)}

\subsubsection{Binomial MGF}
We have $f(x) = \binom{n}{x} p^{x} (1-p)^{n-x}$. The MGF can be found by:

\begin{align}
    M_{X} (t) &= \sum_{x=0}^{n} \binom{n}{x} (\underbrace{e^{t}p}_{a})^{x} (\underbrace{1-p}_b)^{n-x} \\ \nonumber
    &= (e^{t}p + 1-p)^{n}
\end{align}

\noindent by using the fact that 

\begin{equation*}
    \sum_{x} \binom{n}{x} a^{x} b^{n-x} = (a + b)^{n} 
\end{equation*}

\noindent and from which we can obtain $\mathbb{E}(X) = M'_X(0) = n \overbrace{(e^{t}p + 1 - p)^{n-1} \cdot e^{t}p \mid_{t=0}}^{p} = np$

\subsubsection{Poisson MGF}
For a Poisson r.v., where $X \sim \text{Poisson}(\lambda)$ We have $f(x) = e^{-\lambda} \frac{\lambda^{x}}{x!}$. Then,

\begin{align}
    M_X (t) &= \sum_{x=0}^{\infty} e^{tx} e^{-\lambda} \frac{\lambda^{x}}{x!} \\ \nonumber
    &= e^{-\lambda} \sum_{x=0}^{\infty} \frac{(e^{t}\lambda)^{x}}{x!} \\ \nonumber
    &= e^{-\lambda} e^{e^{t}\lambda} \\ \nonumber
    &= e^{e^{t}\lambda - \lambda} \\ \nonumber
    &= e^{\lambda(e^{t} - 1)}
\end{align}

\noindent We can now find:

\begin{equation*}
    M'_X (t) = e^{\lambda(e^{t} - 1)}(\lambda e^{t})
\end{equation*}

\noindent and therefore 

\begin{equation*}
    M'_X (0) = e^{0} (\lambda e^{0}) = \lambda
\end{equation*}

\subsection{Examples (Continuous)}

\subsubsection{Standard Normal}

If $Z \sim \mathcal{N}(0,1)$ is a standard normal r.v., then $f(x) = \frac{1}{\sqrt{2\pi}} e^{-\frac{x^{2}}{2}}$. For continuous distributions, we need to use the infinite integral:

\begin{align}
    M_Z(t) &= \mathbb{E}(e^{tZ}) = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} e^{tx} e^{-\frac{x^{2}}{2}} \, \mathrm{d}x \\ 
    &=  
\end{align}

\section{Gamma Distribution}
The Gamma distribution is a \textbf{continuous distribution} on the positive real line, and is a generalisation of the Exponential distribution. While an Exponential r.v. represents the waiting time for the first success under conditions of \textbf{memorylessness}, the Gamma r.v. represents the total waiting time for \textit{multiple successes}. 

\subsection{Gamma Function}

\begin{definition}
    The \textit{gamma function} $\Gamma$ is defined by \begin{equation}
        \Gamma (a) = \int_{0}^{\infty} x^{a}e^{-x} \, \frac{\mathrm{dx}}{x}
    \end{equation} for real numbers $a> 0$. It is possible to write the integrand as $x^{a-1}e^{-x}$, but it is left for convenience when we make the transformation $u = cx$, so that we have $\frac{du}{u} = \frac{dx}{x}$. 
\end{definition}

\noindent Some properties of the gamma function include: \begin{enumerate}
    \item $\Gamma (a + 1) = a \Gamma (a) \; \forall a > 0$. This follows from integration by parts: \begin{equation*}
        % \Gamma (a+1) = \int_{0}^{\infty} x^{a}e^{-x} \, \mathrm{d}x = -xe^{-x} \Biggr_0^{\infty} + a \int_{0}^{\infty} x^{a-1}e^{-x} \, \mathrm{d}x = 0 + a \Gamma (a)
    \end{equation*}
    \item $\Gamma (n) = (n-1)!$ if $n$ is a positive integer. Can be proved via induction, starting with $n=1$ and using the recursive relation $\Gamma (a+1) = a \Gamma$. 
\end{enumerate}

\subsection{Gamma Distribution}
\begin{definition}
    An r.v. $Y$ is said to have the \textit{Gamma distribution} with parameters $\alpha$ and $\lambda$, where $a>0$ and $\lambda >0 $, if its PDF is: \begin{equation}
        f(x) = \begin{cases}
            \displaystyle\frac{\lambda^{\alpha}}{\Gamma(\alpha)}x^{\alpha-1}e^{-\lambda x}, & \text{if} \; x\geq 0, \\ 
            0, & \text{if} \; x< 0.
        \end{cases}
    \end{equation} We denote this by $X \sim gamma (\alpha, \lambda)$. We have: \begin{equation*}
        \mathbb{E}(X) = \frac{\alpha}{\lambda}, \qquad \text{Var}(X) = \frac{\alpha}{\lambda^{2}}
    \end{equation*}
\end{definition}

\noindent Taking $\alpha=1$, the $\text{gamma}(1,\lambda)$ PDF is $f(x) = \lambda e^{-\lambda x}$, so $\text{gamma}(1, \lambda)$ and $\text{exp}(\lambda)$ are the same. The extra parameter $a$ allows Gamma PDFs to have a greater variety of shapes, refer to Figure \ref{7-gammadist} below. 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{Images/Gammadist.png}
    \caption{Gamma PDFs for various values of $a$ and $\lambda$.}
    \label{fig:7-gammadist}
\end{figure} 

\noindent For small values of $a$, the PDF is skewed, but as $a$ increases, the PDF starts to look more symmetrical and bell-shaped (following the LLN). The mean and variance are increasing in $a$ and decreasing in $\lambda$. 

\subsection{MGF of Gamma Distribution}
In week 3 lecture 1, we proved the MGF of the Gamma distribution: \begin{align*}
    M_X (t) &= \mathbb{E}(e^{tX}) \\ 
    &= \dots \\ 
    &= \frac{\lambda^{\alpha}(\lambda - t)^{-\alpha})}{\Gamma (a)} \underbrace{\int_{0}^{\infty} y^{\alpha-1}e^{-y} \, \mathrm{d}y}_{\Gamma (\alpha)} \\ 
    &= \lambda^{\alpha}(\lambda - t)^{-\alpha}
\end{align*}

\noindent In the special case where $a$ is an integer, we can represent a $\text{gamma}(\alpha, \lambda)$ r.v. as a sum (convolution) of i.i.d. $\text{exp}(\lambda)$ r.v.s. 

\begin{theorem}\label{thm:7-gammaexp}
    Let $X_{1}, X_{2}, \dots , X_n$ be i.i.d. $\text{exp}(\lambda)$. Then \begin{equation*}
        X_{1} + X_{2} +\dots + X_n \sim \text{gamma}(,n\lambda)
    \end{equation*} Since $\alpha = n \in \mathbb{Z}^{+}$, then $\lambda^{\alpha}(\lambda - t)^{-\alpha} = \left(\frac{\lambda}{(\lambda - t)}\right)^{n}$. 
\end{theorem}

\noindent Theorem \ref{thm:7-gammaexp} also allows us to connect the Gamma distribution to the story of the Poisson process. In Poisson processes of rate $\lambda$, the interarrival times are i.i.d. $\text{exp}(\lambda)$ r.v.s but the total waiting time $T_n$ for the $n \text{th}$ arrival is the sum of the first $n$ interarrival times, as shown in Figure \ref{fig:7-poissonprocess} below. $T_3$ is the sum of the 3 interarrival times $X_{1}, X_{2}, X_{3}$. Therefore by the theorem, $T_n \sim \text{gamma}(n, \lambda)$. The interarrival times in a Poisson process are Exponential r.v.s, while the raw arrival times are Gamma r.v.s. 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{Images/arrivaltimes.png}
    \caption{Poisson process, interarrival times $X_j$ are i.i.d. $\text{exp}(\lambda)$, while raw arrival times $T_j$ are $\text{gamma}(j, \lambda)$.}
    \label{fig:7-poissonprocess}
\end{figure} 

\noindent Note that unlike the $X_j$'s, the $T_j$'s are \textbf{not independent}, since they are constrained to be increasing, nor are they i.i.d. Now, we have an interpretation for the parameters of the $\text{gamma}(\alpha, \lambda)$ distribution. In the Poisson process story, $\alpha$ is the \textit{number of successes} we are waiting for, and $\lambda$ is the rate at which successes arrive. $Y \sim \text{gamma}(\alpha, \lambda)$ is the total waiting time for the $a \text{th}$ arrival in a Poisson process of rate $\lambda$.

\section{Conditionals}

\subsection{Bayes' Theorem Recap}
\begin{theorem}
    
    Recall Bayes' Theorem, which provides a link between $ \mathbb{P}(A | B)$ and $ \mathbb{P} (B |A )$: \begin{equation}
        \mathbb{P}(A|B) = \frac{ \mathbb{P}(B |A ) \mathbb{P}(A)}{\mathbb{P}(B)} 
    \end{equation} where $ \mathbb{P}(B)$ is often computed from the \textbf{law of total probability}; for instance, when conditioned on $A$ and $A^{c}$: \begin{equation*}
        \mathbb{P}(B) = \mathbb{P} (B | A) \mathbb{P} (A) + \mathbb{P}(B|A^{c}) \mathbb{P} (A^{c})
    \end{equation*}
\end{theorem}

\subsection{Law of Total Probability}

\begin{definition}
    Let $A_{1}, \dots , A_n$ be a partition of the sample space $S$ (i.e. the $A_i$ are disjoint events and their union is $S$), with $ \mathbb{P}(A_i) > 0, \; \forall i$. Then: \begin{equation}
        \mathbb{P}(B) = \sum_{i=1}^{n} \mathbb{P}(B|A_i)\mathbb{P}(A_i)
    \end{equation}
    The law of total probability tells us that to get the unconditional probability of $B$, we can divide the sample space into disjoint slices $A_i$, find the conditional probability of $B$ within each of the slices, then take a weighted sum of the conditional probabilities, where the weights are probabilities $ \mathbb{P}(A_i)$. 
\end{definition}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{Images/lawoftotalprobability.png}
    \caption{The $A_i$ partition the sample space, $ \mathbb{P}(B)$ is equal to $ \sum_{i} \mathbb{P}(B \cap A_i)$}
    \label{fig:8-lawoftotalprob}
\end{figure} 

\subsection{Independence of Events}

The situation where events provide no information about each other is called independence. 

\begin{definition}
    Events $A$ and $B$ are \textit{independent} if \begin{equation*}
        \mathbb{P}(A \cap B) = \mathbb{P}(A) \mathbb{P}(B)
    \end{equation*} If $ \mathbb{P}(A) > 0$ and $ \mathbb{P}(B) > 0 $, then this is equivalent to \begin{equation*}
        \mathbb{P}(A | B) = \mathbb{P}(A), \qquad \mathbb{P}(B | A) = \mathbb{P}(B)
    \end{equation*}
\end{definition}

Note that independence \footnote{Independence is completely different from \textit{disjointness}. If $A$ and $B$ are disjoint, then $ \mathbb{P}(A \cap B) = 0$, so disjoint events can be independent only if $ \mathbb{P}(A) = 0$ or $ \mathbb{P}(B) = 0$. KNowing that $A$ occurs tells us that $B$ definitely did not occur, so $A$ clearly conveys information about $B$, meaning the two events are not independent.} is a \textit{symmetric relation}: if $A$ is independent of $B$, then $B$ is independent of $A$.

\subsection{Conditional Independence}

\begin{definition}
    Events $A$ and $B$ are said to be \textit{conditionally independent} given $E$ if: \begin{equation}
        \mathbb{P}(A \cap B | E) = \mathbb{P} (A | E) \mathbb{P} (B | E)
    \end{equation} In particular, \begin{enumerate}
        \item Two events can be conditionally independent given $E$, but not given $E^{c}$.
        \item Two events can be conditionally independent given $E$, but not independent. 
        \item Two events can be independent, but not conditionally independent given $E$. 
    \end{enumerate}  
\end{definition} In particular, $ \mathbb{P}(A, B) = \mathbb{P}(A) \mathbb{P}(B)$ \textbf{does not} imply $ \mathbb{P}(A, B | E) = \mathbb{P} (A |E ) \mathbb{P}(B|E)$

\subsection{Properties of the Conditional}

Conditional probability satisfies all the properties of probability. In particular:

\begin{enumerate}
    \item Conditional probabilities are between 0 and 1
    \item $ \mathbb{P}(S | E) = 1$, $ \mathbb{P}(\emptyset | E) = 0$
    \item If $A_{1}, A_{2}, \dots$ are disjoint, then $ \mathbb{P}(\bigcup_{j=1}^{\infty} A_j | E) = \sum_{j=1}^{\infty} \mathbb{P}(A_j | E) $
    \item $ \mathbb{P}(A^{c}|E) = 1 - \mathbb{P}(A | E)$
    \item \textbf{Inclusion Exclusion}: $ \mathbb{P}(A \cap B | E) = \mathbb{P}(A | E) + \mathbb{P}(B|E) - \mathbb{P}(A \cup B | E )$.
\end{enumerate} 

\subsection{Discrete: Conditional P.M.F}

\subsubsection{Joint CDF}

The most general description of the joint distribution of two r.v.s is the joint CDF. 

\begin{definition}
    The joint CDF of r.v.s $X$ and $Y$ is the function $F_{X, Y}$ given by \begin{equation}
        F_{X, Y} (x, y) = \mathbb{P}(X\leq x, Y \leq y)
    \end{equation}
\end{definition}

\subsubsection{Joint PMF}

\begin{definition}
    The joint PMF of discrete r.v.s $X$ and $Y$ is the function \begin{equation}
        p_{X, Y} (x,y) = \mathbb{P}(X = x, Y = y)
    \end{equation} 
\end{definition}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{Images/discretejointpmf.png}
    \caption{Joint PMF of discrete r.v.s $X$ and $Y$}
    \label{fig:}
\end{figure} 

\subsubsection{Marginal PMF}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.4\textwidth]{Images/discretemarginalpmf.png}
    \caption{The marginal PMF $ \mathbb{P}(X = x)$ is obtained by summing over the joint PMF in the $y$-direction.}
    \label{fig:}
\end{figure} 


\subsubsection{Conditional PMF}

\begin{definition}
    For discrete r.v.s $X$ and $Y$, if $f_Y(y) > 0$, then the \textbf{conditional pmf} of $X$ given $Y = y$ is \begin{equation}
        f_{X | Y} (x|y) \coloneq \mathbb{P}((X = x) | (Y = y)) = \frac{f(x, y)}{f_Y (y)}
    \end{equation} This is viewed as a function of $y$ for fixed $x$. Think of $f_{X | Y}(x|y)$ as a function of $x$, when $Y$ is fixed at $y$. It must be the case that 
\end{definition}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{Images/discreteconditionalpmf.png}
    \caption{Conditional pmf of $Y$ given $X = x$. The conditional pmf $ \mathbb{P}(Y = y | X = x)$ is obtained by renormalising the column of the joint pmf that is compatible with the event $X = x$. }
    \label{fig:}
\end{figure} 

\noindent Figure illustrates the 


\noindent We can also relate the conditional distribution to Bayes' theorem, which takes the form \begin{equation*}
    f_{X | Y} (x|y) = \frac{f_{Y | X}(y|x)}{f_Y (y)}
\end{equation*} and if $X$ and $Y$ are independent, then $\forall x, y$ with $f_Y (y) > 0$, we have \begin{equation*}
    f_{X|Y}(x|y) = \frac{f_X(x)f_Y(y)}{f_Y(y)} = f_X (x)
\end{equation*}


\subsection{Continuous: Conditional P.M.F}

\subsubsection{Conditional PDF}

\begin{definition}
    
    For continuous r.v.s $X$ and $Y$, if $f_Y (y) > 0$, then the \textbf{conditional pdf} of $X$ given $Y$ given $Y = y$ is \begin{equation}
        f_{X | Y}(x|y) \coloneq \frac{f(x,y)}{f_Y(y)}
    \end{equation} 
\end{definition}




\section*{References}

Some references used in these notes: \\ 

Introduction to Probability, Joe Blitzstein \& Jessica Hwang. 

\end{document}