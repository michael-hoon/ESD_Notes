\documentclass{article}
\usepackage{LectureNotes}
\usepackage{mathtools}

\setstretch{1.2}

\begin{comment}
\geometry{
    textheight=9in,
    textwidth=5.5in,
    top=1in,
    headheight=12pt,
    headsep=25pt,
    footskip=30pt
}
\end{comment}


\geometry
{
    a4paper,
    total={170mm,257mm},
    left=20mm,
    top=20mm,
}


% ------------------------------------------------------------------------------

\begin{document}

% ------------------------------------------------------------------------------
% Cover Page and ToC
% ------------------------------------------------------------------------------

\title{ \normalsize \textsc{}
		\\ [2.0cm]
		\HRule{1.5pt} \\
		\LARGE \textbf{\uppercase{40.017 Probability \& Statistics}
		\HRule{2.0pt} \\ [0.6cm] \LARGE{Lecture Notes} \vspace*{10\baselineskip}}
		}
\date{\today}
\author{\textbf{Michael Hoon}}

\maketitle
\newpage

\tableofcontents
\newpage

% ------------------------------------------------------------------------------


\section{Set Theory}

\subsection{Sample Spaces}

The mathematical framework for probability is built around \textit{sets}. The \textit{sample space} $S$ of an experiment is the set of all possible outcomes of the experiment. An \textit{event} $A$ is a subset of $S$, and we say that $A$ occurred if the actual outcome is in $A$.  

\subsection{Naive Definition of Probability}

Let $A$ be an event for an experiment with a finite sample space $S$. A naive probability of $A$ is 

\begin{equation}
    \mathbb{P}_{\text{naive}}(A) = \frac{|A|}{|S|} = \frac{\text{number of outcomes favorable to A}}{\text{total number of outcomes}}
\end{equation}

\noindent In general, the result about complements always holds:

\begin{equation*}
    \mathbb{P}_{\text{naive}}(A^{c}) = \frac{|A^{c}|}{|S|} = \frac{|S| - |A|}{|S|} = 1- \frac{|A|}{|S|} = 1 - \mathbb{P}_{\text{naive}}(A)
\end{equation*}

An important factor about the naive definition is that it is restrictive in requiring $S$ to be finite. 

\subsection{General Definition of Probability}

\begin{definition}
    A probability space consists of a sample space $S$ and a probability function $P$ which takes an event $A \subseteq S$ as input and returns $P(A)$, where $P(A) \in \mathbb{R}, \; P(A) \in [0, 1]$. The function must satisfy the following axioms:

    \begin{enumerate}
        \item $\mathbb{P}(\emptyset) = 1, \; \mathbb{P}(S) = 1$ 
        \item $\mathbb{P}(A) \geq 0$
        \item If $A_{1}, A_{2}, \dots$ are \textbf{disjoint events}, then: \begin{equation*}
            \mathbb{P} \left( \bigcup_{j=1}^{\infty} \right) = \sum_{j=1}^{\infty} \mathbb{P}(A_j)
        \end{equation*}
        \noindent Disjoint events are \textbf{mutually exclusive} (i.e. $A_i \cap A_j = \emptyset \; \forall \; i \neq j$).
    \end{enumerate}
\end{definition}

\subsubsection{Properties of Probability}
\begin{theorem}

Probability has the following properties, for any events $A$ and $B$: 

\begin{enumerate}
    \item $\mathbb{P}(A^{c}) = 1 - \mathbb{P}(A)$
    \item If $A \subseteq B$, then $\mathbb{P}(A) \leq \mathbb{P}(B)$
    \item $\mathbb{P}(A \cup B) = \mathbb{P}(A) + \mathbb{P} (B) - \mathbb{P}(A \cap B)$ 
\end{enumerate}
\end{theorem}

\subsubsection{Inclusion-Exclusion Principle}

For any events $A_{1}, \dots A_n$, 

\begin{equation}
    \mathbb{P}\left( \bigcup_{i=1}^{n} A_i \right) = \sum_{i} \mathbb{P}(A_i) - \sum_{i<j} \mathbb{P}(A_i \cap A_j) + \sum_{i<j<k} \mathbb{P}(A_i \cap A_j \cap A_k) - \dots + (-1)^{n+1} \mathbb{P}(A_{1} \cap \dots \cap A_n)
\end{equation}

\noindent For $n=2$, we have a nicer result:

\begin{equation*}
    \mathbb{P}(A_{1} \cup A_{2}) = \mathbb{P}(A_{1}) + \mathbb{P}(A_{2}) - \mathbb{P}(A_{1} \cap A_{2})
\end{equation*}

\subsection{Conditional Probability}

\begin{definition}
    If $A$ and $B$ are events with $\mathbb{P}(B) > 0$, then the \textit{conditional probability} of $A$ given $B$, denoted by $\mathbb{P}(A \mid B)$ is defined as: 

    \begin{equation*}
        \mathbb{P}(A | B) = \frac{\mathbb{P}(A \cap B)}{\mathbb{P}(B)}
    \end{equation*}
\end{definition}

\noindent Here $A$ is the event whose uncertainty we want to update, and $B$ is the evidence we observe. $\mathbb{P}(A)$ is the \textit{prior} probability of $A$ and $\mathbb{P}(A | B)$ is the \textit{posterior} probability of $A$. (For any event $A$, $\mathbb{P}(A|A) = \frac{\mathbb{P}(A \cap A)}{\mathbb{P}(A)}$).




\section{Derangement}

A derangement is a permutation of the elements of a set in which no element appears in its original position. We use $D_n$ to denote the number of derangements of $n$ distinct objects. 

\subsection{Counting Derangements}
We consider the number of ways in which $n$ hats ($h_{1}, \dots, h_n$) can be returned to $n$ people ($P_{1}, \dots, P_n$) such that no hat makes it back to its owner. \\

We obtain the recursive formula:

\begin{equation}\label{1-eq: derangement}
    D_n = (n-1)(D_{n-1} + D_{n-2}), \; \forall \; n \geq 2
\end{equation}

\noindent With the initial conditions $D_{1} = 0$ and $D_{2} = 1$, we can use the formula to recursively compute $D_n$ for any $n$. \\

\noindent There are various other expressions for $D_n$, equivalent to formula \ref{1-eq: derangement}:

\begin{equation}\label{1-eq: derangement sum}
    D_n = n! \sum_{i=0}^{n} \frac{(-1)^{i}}{i!}, \; \forall \; n \geq 0
\end{equation}

\subsubsection{Limiting Growth}
From Equation \ref{1-eq: derangement sum}, and the taylor series expansion for $e$:

\begin{equation}
    e^{x} = \sum_{i=0}^{\infty} \frac{x^{i}}{i!} 
\end{equation}

\noindent we substitute $x=-1$ and obtain the limiting value as $n \to \infty$:

\begin{equation*}
    \lim_{n \to \infty} \frac{D_n}{n!} = \lim_{n \to \infty} \sum_{i=0}^{n} \frac{(-1)^{i}}{i!} = e^{-1} \approx 0.367879\dots
\end{equation*}

\noindent This is the limit of the probability that a randomly selected permutation of a large number of objects is a derangement. The probability converges to this limit extremely quickly as $n$ increases, which is why $D_n$ is the nearest integer to $\frac{n!}{e}$. 

\section{Discrete Random Variables}

We formally define a random variable:

\begin{definition}
    Given an experiment with sample space $S$, a \textit{random variable} (r.v.) is a function from the sample space $S$ to the real numbers $\mathbb{R}$. It is common to denote random variables by capital letters. 
\end{definition}

\noindent Thus, a random variable $X$ assigns a numerical value $X(s)$ to each possible outcome $s$ of the experiment. The randomness comes from the fact that we have a random experiment (with Probabilities described by the probability function $P$); the mapping itself is deterministic. \\

There are two main types of random variables used in practice: \textit{discrete} and \textit{continuous} r.v.s. 

\begin{definition}
    A random variable $X$ is said to be \textit{discrete} if there is a finite list of values $a_{1}, a_{2}, \dots, a_n$ or an infinite list of values $a_{1}, a_{2}, \dots$ such that $\mathbb{P}(X = a_j \; \text{for some }j) = 1$. If $X$ is a discrete r.v., then the finite or countably infinite set of values $x$ such that $P(X = x) > 0$ is called the \textit{support} of $X$.  
\end{definition}







\subsection{Binomial}

\subsection{Hypergeometric}

If we have an urn filled with $w$ white and $b$ black balls, then drawing $n$ balls out of the urn \textit{with replacement} yields a $\text{Binom}(n, \frac{w}{(w+b)})$. If we instead sample \textit{without replacement}, then the number of white balls follow a \textbf{Hypergeometric} distribution. 

\begin{theorem}
    If $X \sim \text{hypgeo}(n, j, k)$, then the PMF of $X$ is:

    \begin{equation*}
        \mathbb{P}(X = x) = \frac{\binom{j}{x}\binom{k}{n-x}}{\binom{j+k}{n}}
    \end{equation*}
    \noindent $\forall x \in \mathbb{Z}$ satisfying $0\leq x \leq n$ and $0\leq n-x \leq j$, and $P(X=x) = 0$ otherwise. 
\end{theorem}

\noindent If $j$ and $k$ are large compared to $n$, then selection without replacement can be approximated by selection with replacement. In that case, the hypergeometric RV $X \sim \text{hypgeo}(n,j,k)$ can be approximated by a binomial RV $Y \sim \text{binomial}(n,p)$, where $p := \frac{j}{j+k}$ is the probability of selecting a black marble. \\

\noindent We can also write $X$ as the sum of (dependent) Bernoulli random variables:

\begin{equation*}
    X = X_{1} + X_{2} + \dots + X_n
\end{equation*}

where each $X_i$ equals 1 if the $i$th selected marble is black, and 0 otherwise. 


\subsubsection{Hypergeometric Symmetry}

\begin{theorem}
    The hypergeo$(w,b,n)$ and hypergeo$(n,w+b-n,w)$ distributions are identical. 
\end{theorem}

\noindent The proof follows from swapping the two sets of tags in the Hypergeometric story (white/black balls in urn) \footnote[3]{The binomial and hypergeometric distributions are often confused. Note that in Binomial distributions, the Bernoulli trials are \textbf{independent}. The Bernoulli trials in Hypergeometric distribution are \textbf{dependent}, since the sampling is done \textit{without replacement}.}. 


\subsection{Geometric}

\subsection{Negative Binomial}

In a sequence of independent Bernoulli trials with success probability $p$, if $X$ is the number of failures before the $r$th success, then $X$ is said to have the Negative Binomial distribution with parameters $r$ and $p$, denoted $X \sim \text{NBin}(r,p)$. \\ 

\noindent Both the Binomial and Negative Binomial distributions are based on independent Bernoulli trials; they differ in the \textit{stopping rule} and in what they are counting. The Negative Binomial counts the \textbf{number of failures until a fixed number of successes}. 

\begin{theorem}
    If $X \sim \text{NBin}(r,p)$, then the PMF of $X$ is 

    \begin{equation}
        P(X=x) = \binom{x-1}{n-1} (1-p)^{x-n}p^{n}, \; \forall \; x\geq n 
    \end{equation}
\end{theorem}





\section{Law of Large numbers}
Assume that we have i.i.d. $X_{1}, X_{2}, \dots$ with finite mean $\mu$ and finite variance $\sigma^{2}$. Let $\bar{X}_n = \frac{1}{n} \sum_{i=1}^{n} X_i$.

\begin{definition}
    The (Weak) Law of Large Numbers (LLN) says that as $n$ grows, the sample mean $\bar{X}_n$ converges to the true mean $\mu$. Mathematically, 
    \begin{equation}
        \forall \epsilon > 0, \; \mathbb{P}(|\bar{X}_n - \mu < \epsilon) = 1, \; \text{as} \; n \to \infty 
    \end{equation}

    \noindent For any positive margin $\epsilon$, as $n$ gets arbitrarily large, the probability that $\bar{X}_n$ is within $\epsilon$ of $\mu$ approaches 1. 
\end{definition}

\noindent Note that the LLN does not contradict the fact that a coin is memoryless (in the repeated coin toss experiment). The LLN states that the proportion of Heads converges to $\frac{1}{2}$, but this does not imply that after a long string of Heads, the coin is "due" for a Tails to "\textit{balance things out}". Rather, the convergence takes place through \textit{swamping}: past tosses are swamped by the infinitely many tosses that are yet to come.  

\subsection{Inequalities}
The inequalities in this section provide bounds on the probability of an r.v. taking on an 'extreme' value in the right or left rail of a distribution. 

\subsubsection{Markov's Inequality}
\begin{definition}
    Let $X$ be any random variable that takes only non-negative values, that is, $\mathbb{P}(X < 0) = 0$. Then for any constant $a >0$, we have: 
    \begin{equation}
        \mathbb{P}(X \geq a) \leq \frac{\mathbb{E}(X)}{a}
    \end{equation}
\end{definition}

\noindent For an intuitive interpretation, let $X$ be the income of a randomly selected individual from a population. Taking $a = \mathbb{E}(X)$, Markov's Inequality says that $\mathbb{P(X \geq 2 \mathbb{E}(X)) \leq \frac{1}{2}}$. i.e., it is impossible for more than half the population to make at least twice the average income. 

\subsubsection{Chebyshev's Inequality}
Gives general bounds for the probability of being $k$ standard deviations (SD) away from the mean. 

\begin{definition}
    Let $Y$ be any random variable with mean $\mu < \infty$ and variance $\sigma^{2} > 0$. Then for any constant $k>0$, we have: 
    \begin{equation}
        \mathbb{P}(|Y-\mu| \geq k\sigma) \leq \frac{1}{k^{2}}
    \end{equation} 
\end{definition}

\section{Central Limit Theorem}
Let $X_{1}, X_{2}, \dots$ be i.i.d. with mean $\mu$ and variance $\sigma^{2}$. 

\begin{definition}
    The CLT states that for large $n$, the distribution of $\bar{X}_n$ after standardisation approaches a standard Normal distribution. By standardisation, we mean that we subtract $\mu$, the mean of $\bar{X}_n$, and divide by $\frac{\sigma}{\sqrt{n}}$, the standard deviation of $\bar{X}_n$. 
    \begin{equation}
        \lim_{n \to \infty} \mathbb{P}(\frac{\bar{X}_n - \mu}{\frac{\sigma}{\sqrt{n}}} \leq x) = \Phi(x)    
    \end{equation}
    \noindent which is the cdf of the standard normal. Informally, when $n$ is large ($\geq 30$), then $\bar{X}_n$ and $\sum_{i=1}^{n} X_i$ can each be approximated by a normal RV with the same mean and variance; the actual distribution of $X_i$ becomes irrelevant:
    \begin{equation*}
        \bar{X}_n \approx N(\mu, \frac{\sigma^{2}}{n}), \qquad \sum_{i=1}^{n} X_i \approx N(n\mu, n\sigma^{2})
    \end{equation*}
\end{definition}

\section{Moments}

\subsection{Interpreting Moments}
\begin{definition}
    Let $X$ be an r.v. with mean $\mu$ and variance $\sigma^{2}$. For any positive integer $n$, the $n^{\text{th}}$ moment of $X$ is $\mathbb{E}(X^{n})$, the $n^{\text{th}}$ central moment is $\mathbb{E}((X - \mu)^{n})$. 
\end{definition}

\noindent In particular, the mean is the first moment and the variance is the second central moment. 

\subsection{Moment Generating Functions}
A moment generating function, as its name suggests, is a generating function that encodes the \textbf{moments} of a distribution. Starting with an infinite sequence ($a_{0}, a_{1}, a_{2}, \dots$), we 'condense' or 'store' it as a single function $g$, the generating function of the sequence:

\begin{equation*}
    \sum_{n=0}^{\infty} a_n \frac{t^{n}}{n!} \coloneqq g(t) 
\end{equation*}

\begin{definition}
    When we take $a_n = \mathbb{E}(X^{n})$, the resulting generating function is known as the \textbf{moment generating function (MGF)} of $X$, and is denoted by $M_X(t)$. \\ 

    \noindent The MGF of $X$ can be computed as an expected value: 

    % \begin{equation}
    %     M_X (t) = \sum_{n=0}^{\infty} \mathbb{E(X^{n}) \frac{t^{n}}{n!} = \sum_{n=0}^{\infty} \mathbb{E}\left(\frac{t^{n}}{n!}\right) \\
    %     = \mathbb{E}\left(\sum_{n=1}^{\infty}\frac{(tX)^{n}}{n!}\right) \\
    %     = \mathbb{E}(e^{tX})
    % \end{equation}
\end{definition}

\noindent Note that $M_X (0) = 1$ for any valid MGF. 

\subsection{Formulas \& Theorems}
Some important formulas for the MGF of $X$:

\begin{equation}
    \boxed{M_X (t) = \mathbb{E} (e^{tX})} 
\end{equation}

\noindent where if $X$ is \textbf{discrete} with pmf $f$, then 

\begin{equation}
    M_X (t) = \sum_{all x_i} e^{t x_i} f(x_i) 
\end{equation}

\noindent and if $X$ is \textbf{continuous} with pdf $f$, then 

\begin{equation}
    M_X (t) = \int_{-\infty}^{\infty} e^{tx} f(x) \, \mathrm{d}x
\end{equation}

\begin{theorem}
    Given the MGF of $X$, we can get the $n^{\text{th}}$ moment of $X$ by evaluating the $n^{\text{th}}$ derivative of the MGF at 0: 

    \begin{equation}
        \boxed{\mathbb{E}(X^{n}) = M_X^{(N)} (0)} 
    \end{equation}
\end{theorem}

\begin{theorem}
    If $X$ and $Y$ are independent, then the MGF of $X + Y$ is the product of the individual MGFs:
    \begin{equation}
        M_{X+Y} (t) = M_X(t)M_Y(t)
    \end{equation}
    \noindent This is true because if $X$ and $Y$ are independent, then $\mathbb{E}(e^{t(X+Y)}) = \mathbb{E}(e^{tX})\mathbb{E}(e^{tY})$ 
\end{theorem}

\begin{theorem}
    If two random variables have the same MGF, then they have the same distribution (same cdf, equivalently, same pdf or pmf) \footnote{For this to apply, the MGF needs to exist in an open interval around $t=0$}.
\end{theorem}

\subsection{Examples (Discrete)}

\subsubsection{Binomial MGF}
We have $f(x) = \binom{n}{x} p^{x} (1-p)^{n-x}$. The MGF can be found by:

\begin{align}
    M_{X} (t) &= \sum_{x=0}^{n} \binom{n}{x} (\underbrace{e^{t}p}_{a})^{x} (\underbrace{1-p}_b)^{n-x} \\ \nonumber
    &= (e^{t}p + 1-p)^{n}
\end{align}

\noindent by using the fact that 

\begin{equation*}
    \sum_{x} \binom{n}{x} a^{x} b^{n-x} = (a + b)^{n} 
\end{equation*}

\noindent and from which we can obtain $\mathbb{E}(X) = M'_X(0) = n \overbrace{(e^{t}p + 1 - p)^{n-1} \cdot e^{t}p \mid_{t=0}}^{p} = np$

\subsubsection{Poisson MGF}
For a Poisson r.v., where $X \sim \text{Poisson}(\lambda)$ We have $f(x) = e^{-\lambda} \frac{\lambda^{x}}{x!}$. Then,

\begin{align}
    M_X (t) &= \sum_{x=0}^{\infty} e^{tx} e^{-\lambda} \frac{\lambda^{x}}{x!} \\ \nonumber
    &= e^{-\lambda} \sum_{x=0}^{\infty} \frac{(e^{t}\lambda)^{x}}{x!} \\ \nonumber
    &= e^{-\lambda} e^{e^{t}\lambda} \\ \nonumber
    &= e^{e^{t}\lambda - \lambda} \\ \nonumber
    &= e^{\lambda(e^{t} - 1)}
\end{align}

\noindent We can now find:

\begin{equation*}
    M'_X (t) = e^{\lambda(e^{t} - 1)}(\lambda e^{t})
\end{equation*}

\noindent and therefore 

\begin{equation*}
    M'_X (0) = e^{0} (\lambda e^{0}) = \lambda
\end{equation*}

\subsection{Examples (Continuous)}

\subsubsection{Standard Normal}

If $Z \sim \mathcal{N}(0,1)$ is a standard normal r.v., then $f(x) = \frac{1}{\sqrt{2\pi}} e^{-\frac{x^{2}}{2}}$. For continuous distributions, we need to use the infinite integral:

\begin{align}
    M_Z(t) &= \mathbb{E}(e^{tZ}) = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} e^{tx} e^{-\frac{x^{2}}{2}} \, \mathrm{d}x \\ 
    &=  
\end{align}

\section{Gamma Distribution}
The Gamma distribution is a \textbf{continuous distribution} on the positive real line, and is a generalisation of the Exponential distribution. While an Exponential r.v. represents the waiting time for the first success under conditions of \textbf{memorylessness}, the Gamma r.v. represents the total waiting time for \textit{multiple successes}. 

\subsection{Gamma Function}

\begin{definition}
    The \textit{gamma function} $\Gamma$ is defined by \begin{equation}
        \Gamma (a) = \int_{0}^{\infty} x^{a}e^{-x} \, \frac{\mathrm{dx}}{x}
    \end{equation} for real numbers $a> 0$. It is possible to write the integrand as $x^{a-1}e^{-x}$, but it is left for convenience when we make the transformation $u = cx$, so that we have $\frac{du}{u} = \frac{dx}{x}$. 
\end{definition}

\noindent Some properties of the gamma function include: \begin{enumerate}
    \item $\Gamma (a + 1) = a \Gamma (a) \; \forall a > 0$. This follows from integration by parts: \begin{equation*}
        % \Gamma (a+1) = \int_{0}^{\infty} x^{a}e^{-x} \, \mathrm{d}x = -xe^{-x} \Biggr_0^{\infty} + a \int_{0}^{\infty} x^{a-1}e^{-x} \, \mathrm{d}x = 0 + a \Gamma (a)
    \end{equation*}
    \item $\Gamma (n) = (n-1)!$ if $n$ is a positive integer. Can be proved via induction, starting with $n=1$ and using the recursive relation $\Gamma (a+1) = a \Gamma$. 
\end{enumerate}

\subsection{Gamma Distribution}
\begin{definition}
    An r.v. $Y$ is said to have the \textit{Gamma distribution} with parameters $\alpha$ and $\lambda$, where $a>0$ and $\lambda >0 $, if its PDF is: \begin{equation}
        f(x) = \begin{cases}
            \displaystyle\frac{\lambda^{\alpha}}{\Gamma(\alpha)}x^{\alpha-1}e^{-\lambda x}, & \text{if} \; x\geq 0, \\ 
            0, & \text{if} \; x< 0.
        \end{cases}
    \end{equation} We denote this by $X \sim gamma (\alpha, \lambda)$. We have: \begin{equation*}
        \mathbb{E}(X) = \frac{\alpha}{\lambda}, \qquad \text{Var}(X) = \frac{\alpha}{\lambda^{2}}
    \end{equation*}
\end{definition}

\noindent Taking $\alpha=1$, the $\text{gamma}(1,\lambda)$ PDF is $f(x) = \lambda e^{-\lambda x}$, so $\text{gamma}(1, \lambda)$ and $\text{exp}(\lambda)$ are the same. The extra parameter $a$ allows Gamma PDFs to have a greater variety of shapes, refer to Figure \ref{7-gammadist} below. 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{Images/Gammadist.png}
    \caption{Gamma PDFs for various values of $a$ and $\lambda$.}
    \label{fig:7-gammadist}
\end{figure} 

\noindent For small values of $a$, the PDF is skewed, but as $a$ increases, the PDF starts to look more symmetrical and bell-shaped (following the LLN). The mean and variance are increasing in $a$ and decreasing in $\lambda$. 

\subsection{MGF of Gamma Distribution}
In week 3 lecture 1, we proved the MGF of the Gamma distribution: \begin{align*}
    M_X (t) &= \mathbb{E}(e^{tX}) \\ 
    &= \dots \\ 
    &= \frac{\lambda^{\alpha}(\lambda - t)^{-\alpha})}{\Gamma (a)} \underbrace{\int_{0}^{\infty} y^{\alpha-1}e^{-y} \, \mathrm{d}y}_{\Gamma (\alpha)} \\ 
    &= \lambda^{\alpha}(\lambda - t)^{-\alpha}
\end{align*}

\noindent In the special case where $a$ is an integer, we can represent a $\text{gamma}(\alpha, \lambda)$ r.v. as a sum (convolution) of i.i.d. $\text{exp}(\lambda)$ r.v.s. 

\begin{theorem}\label{thm:7-gammaexp}
    Let $X_{1}, X_{2}, \dots , X_n$ be i.i.d. $\text{exp}(\lambda)$. Then \begin{equation*}
        X_{1} + X_{2} +\dots + X_n \sim \text{gamma}(,n\lambda)
    \end{equation*} Since $\alpha = n \in \mathbb{Z}^{+}$, then $\lambda^{\alpha}(\lambda - t)^{-\alpha} = \left(\frac{\lambda}{(\lambda - t)}\right)^{n}$. 
\end{theorem}

\noindent Theorem \ref{thm:7-gammaexp} also allows us to connect the Gamma distribution to the story of the Poisson process. In Poisson processes of rate $\lambda$, the interarrival times are i.i.d. $\text{exp}(\lambda)$ r.v.s but the total waiting time $T_n$ for the $n \text{th}$ arrival is the sum of the first $n$ interarrival times, as shown in Figure \ref{fig:7-poissonprocess} below. $T_3$ is the sum of the 3 interarrival times $X_{1}, X_{2}, X_{3}$. Therefore by the theorem, $T_n \sim \text{gamma}(n, \lambda)$. The interarrival times in a Poisson process are Exponential r.v.s, while the raw arrival times are Gamma r.v.s. 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{Images/arrivaltimes.png}
    \caption{Poisson process, interarrival times $X_j$ are i.i.d. $\text{exp}(\lambda)$, while raw arrival times $T_j$ are $\text{gamma}(j, \lambda)$.}
    \label{fig:7-poissonprocess}
\end{figure} 

\noindent Note that unlike the $X_j$'s, the $T_j$'s are \textbf{not independent}, since they are constrained to be increasing, nor are they i.i.d. Now, we have an interpretation for the parameters of the $\text{gamma}(\alpha, \lambda)$ distribution. In the Poisson process story, $\alpha$ is the \textit{number of successes} we are waiting for, and $\lambda$ is the rate at which successes arrive. $Y \sim \text{gamma}(\alpha, \lambda)$ is the total waiting time for the $a \text{th}$ arrival in a Poisson process of rate $\lambda$.

\section{Conditionals}

\subsection{Bayes' Theorem Recap}
\begin{theorem}
    
    Recall Bayes' Theorem, which provides a link between $ \mathbb{P}(A | B)$ and $ \mathbb{P} (B |A )$: \begin{equation}
        \mathbb{P}(A|B) = \frac{ \mathbb{P}(B |A ) \mathbb{P}(A)}{\mathbb{P}(B)} 
    \end{equation} where $ \mathbb{P}(B)$ is often computed from the \textbf{law of total probability}; for instance, when conditioned on $A$ and $A^{c}$: \begin{equation*}
        \mathbb{P}(B) = \mathbb{P} (B | A) \mathbb{P} (A) + \mathbb{P}(B|A^{c}) \mathbb{P} (A^{c})
    \end{equation*}
\end{theorem}

\subsection{Law of Total Probability}

\begin{definition}
    Let $A_{1}, \dots , A_n$ be a partition of the sample space $S$ (i.e. the $A_i$ are disjoint events and their union is $S$), with $ \mathbb{P}(A_i) > 0, \; \forall i$. Then: \begin{equation}
        \mathbb{P}(B) = \sum_{i=1}^{n} \mathbb{P}(B|A_i)\mathbb{P}(A_i)
    \end{equation}
    The law of total probability tells us that to get the unconditional probability of $B$, we can divide the sample space into disjoint slices $A_i$, find the conditional probability of $B$ within each of the slices, then take a weighted sum of the conditional probabilities, where the weights are probabilities $ \mathbb{P}(A_i)$. 
\end{definition}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{Images/lawoftotalprobability.png}
    \caption{The $A_i$ partition the sample space, $ \mathbb{P}(B)$ is equal to $ \sum_{i} \mathbb{P}(B \cap A_i)$}
    \label{fig:8-lawoftotalprob}
\end{figure} 

\subsection{Independence of Events}

The situation where events provide no information about each other is called independence. 

\begin{definition}
    Events $A$ and $B$ are \textit{independent} if \begin{equation*}
        \mathbb{P}(A \cap B) = \mathbb{P}(A) \mathbb{P}(B)
    \end{equation*} If $ \mathbb{P}(A) > 0$ and $ \mathbb{P}(B) > 0 $, then this is equivalent to \begin{equation*}
        \mathbb{P}(A | B) = \mathbb{P}(A), \qquad \mathbb{P}(B | A) = \mathbb{P}(B)
    \end{equation*}
\end{definition}

Note that independence \footnote{Independence is completely different from \textit{disjointness}. If $A$ and $B$ are disjoint, then $ \mathbb{P}(A \cap B) = 0$, so disjoint events can be independent only if $ \mathbb{P}(A) = 0$ or $ \mathbb{P}(B) = 0$. KNowing that $A$ occurs tells us that $B$ definitely did not occur, so $A$ clearly conveys information about $B$, meaning the two events are not independent.} is a \textit{symmetric relation}: if $A$ is independent of $B$, then $B$ is independent of $A$.

\subsection{Conditional Independence}

\begin{definition}
    Events $A$ and $B$ are said to be \textit{conditionally independent} given $E$ if: \begin{equation}
        \mathbb{P}(A \cap B | E) = \mathbb{P} (A | E) \mathbb{P} (B | E)
    \end{equation} In particular, \begin{enumerate}
        \item Two events can be conditionally independent given $E$, but not given $E^{c}$.
        \item Two events can be conditionally independent given $E$, but not independent. 
        \item Two events can be independent, but not conditionally independent given $E$. 
    \end{enumerate}  
    \noindent Equivalently, we have the result \begin{equation}
        \mathbb{P}(B | A \cap E) = \mathbb{P}(B | E)
    \end{equation}
\end{definition} In particular, $ \mathbb{P}(A, B) = \mathbb{P}(A) \mathbb{P}(B)$ \textbf{does not} imply $ \mathbb{P}(A, B | E) = \mathbb{P} (A |E ) \mathbb{P}(B|E)$

\subsection{Properties of the Conditional}

Conditional probability satisfies all the properties of probability. In particular:

\begin{enumerate}
    \item Conditional probabilities are between 0 and 1
    \item $ \mathbb{P}(S | E) = 1$, $ \mathbb{P}(\emptyset | E) = 0$
    \item If $A_{1}, A_{2}, \dots$ are disjoint, then $ \mathbb{P}(\bigcup_{j=1}^{\infty} A_j | E) = \sum_{j=1}^{\infty} \mathbb{P}(A_j | E) $
    \item $ \mathbb{P}(A^{c}|E) = 1 - \mathbb{P}(A | E)$
    \item \textbf{Inclusion Exclusion}: $ \mathbb{P}(A \cap B | E) = \mathbb{P}(A | E) + \mathbb{P}(B|E) - \mathbb{P}(A \cup B | E )$.
\end{enumerate} 

\subsection{Discrete: Conditional P.M.F}

\subsubsection{Joint CDF}

The most general description of the joint distribution of two r.v.s is the joint CDF. 

\begin{definition}
    The joint CDF of r.v.s $X$ and $Y$ is the function $F_{X, Y}$ given by \begin{equation}
        F_{X, Y} (x, y) = \mathbb{P}(X\leq x, Y \leq y)
    \end{equation}
\end{definition}

\subsubsection{Joint PMF}

\begin{definition}
    The joint PMF of discrete r.v.s $X$ and $Y$ is the function \begin{equation}
        p_{X, Y} (x,y) = \mathbb{P}(X = x, Y = y)
    \end{equation} 
\end{definition}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{Images/discretejointpmf.png}
    \caption{Joint PMF of discrete r.v.s $X$ and $Y$}
    \label{fig:}
\end{figure} 

\subsubsection{Marginal PMF}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.4\textwidth]{Images/discretemarginalpmf.png}
    \caption{The marginal PMF $ \mathbb{P}(X = x)$ is obtained by summing over the joint PMF in the $y$-direction.}
    \label{fig:}
\end{figure} 


\subsubsection{Conditional PMF}

\begin{definition}
    For discrete r.v.s $X$ and $Y$, if $f_Y(y) > 0$, then the \textbf{conditional pmf} of $X$ given $Y = y$ is \begin{equation}
        f_{X | Y} (x|y) \coloneq \mathbb{P}((X = x) | (Y = y)) = \frac{f(x, y)}{f_Y (y)}
    \end{equation} This is viewed as a function of $y$ for fixed $x$. Think of $f_{X | Y}(x|y)$ as a function of $x$, when $Y$ is fixed at $y$. It must be the case that 
\end{definition}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{Images/discreteconditionalpmf.png}
    \caption{Conditional pmf of $Y$ given $X = x$. The conditional pmf $ \mathbb{P}(Y = y | X = x)$ is obtained by renormalising the column of the joint pmf that is compatible with the event $X = x$. }
    \label{fig:}
\end{figure} 

\noindent Figure illustrates the 


\noindent We can also relate the conditional distribution to Bayes' theorem, which takes the form \begin{equation*}
    f_{X | Y} (x|y) = \frac{f_{Y | X}(y|x)}{f_Y (y)}
\end{equation*} and if $X$ and $Y$ are independent, then $\forall x, y$ with $f_Y (y) > 0$, we have \begin{equation*}
    f_{X|Y}(x|y) = \frac{f_X(x)f_Y(y)}{f_Y(y)} = f_X (x)
\end{equation*}


\subsection{Continuous: Conditional P.M.F}

\subsubsection{Conditional PDF}

\begin{definition}
    
    For continuous r.v.s $X$ and $Y$, if $f_Y (y) > 0$, then the \textbf{conditional pdf} of $X$ given $Y$ given $Y = y$ is \begin{equation}
        f_{X | Y}(x|y) \coloneq \frac{f(x,y)}{f_Y(y)}
    \end{equation} 
\end{definition}


\section{Conditional Expectation}

\subsection{Discrete}

The expectation $\mathbb{E}(Y)$ of a discrete r.v. $Y$ is a weighted average of its possible values, where the weights are the PMF values $\mathbb{P}(Y = y)$. After learning that an event $A$ occurred, we want to use weights that have been updated to reflect this new information. 

\begin{definition}
    For \textit{discrete} r.v.s $X$ and $Y$, the \textbf{conditional expectation} of $X$ given $Y = y$ is \begin{equation}
        \mathbb{E}(X | Y = y) \coloneq \sum_{\text{all } x} x \mathbb{P}((X = x) | (Y = y)) = \sum_{\text{all }x }f_{X | Y} (x|y)  
    \end{equation} That is, $\mathbb{E}(X | Y = y)$ is the expectation of $X$ given $Y = y$. 
\end{definition}

\subsubsection{Law of Total Expectation (Discrete)}

\begin{definition}
    We have the law of total expectation: \begin{equation}
        \mathbb{E}(X) = \sum_{\text{all } y} \mathbb{E}(X | Y = y) \mathbb{P}(Y = y) 
    \end{equation}
\end{definition}

\subsection{Continuous}

\begin{definition}
    For \textit{continuous} r.v.s $X$ and $Y$, the \textbf{conditional expectation} of $X$ given $Y = y$: \begin{equation}
        \mathbb{E}(X | Y = y) \coloneq \int_{-\infty}^{\infty} x f_{X | Y} (x | y) \, \mathrm{d}x = \frac{1}{f_Y (y)} \int_{-\infty}^{\infty} xf(x,y) \, \mathrm{d}x
    \end{equation}
\end{definition}

\subsubsection{Law of Total Expectation (Continuous)}

\begin{definition}
    Similarly, we have the law of total expectation: \begin{equation}
        \mathbb{E}(X) = \int_{-\infty}^{\infty} \mathbb{E}(X | Y = y) f_Y (y) \, \mathrm{d}y 
    \end{equation}
\end{definition}

\subsection{An Alternate Formulation}

Notice that because we sum or integrate over $x$, $\mathbb{E}(X | Y = y)$ is a function of $y$ only. Then, we let this be $g(y)$, so $\mathbb{E}(X | Y = y) = g(y)$. Then, the law of total expectation says: \begin{align}
    \mathbb{E}(X) &= \sum_{\text{all } y} \textcolor{red}{g(y)} \mathbb{P}(Y = y) = \mathbb{E}(g(Y)) \\ 
    \mathbb{E}(X) &= \int_{-\infty}^{\infty} \textcolor{red}{g(y)} f_Y (y) \, \mathrm{d} y = \mathbb{E}(g(Y))
\end{align}

\begin{definition}
    Let $g(x) = \mathbb{E}(X | Y = y)$. Then the conditional expectation of $X$ given $Y$, denoted $ \mathbb{E}(X | Y)$ is defined to be the random variable $g(Y)$. Then the law of total expectation can be written: \begin{equation}
        \mathbb{E}(X) = \mathbb{E}(\mathbb{E}(X | Y))
    \end{equation}
    \noindent In other words, if after doing the experiment $X$ crystallises into $x$, then $ \mathbb{E}(X | Y)$ crystallises into $g(y)$. 
\end{definition}


\subsection{Conditional Expectation Given An Event}

For any event $A$, we adapt the \textbf{law of total expectation} to compute $\mathbb{P}(A)$. We first define a random variable $X$, where $X = 1$ if $A$ occurs, and $X = 0$ otherwise. Then, $\mathbb{E}(X) = \mathbb{P}(A)$, $\mathbb{E}(X | Y = y) = \mathbb{P}(A | Y = y)$. 

\begin{theorem}
    We apply the law to $\mathbb{E}(X)$ to obtain: \begin{equation}
        \mathbb{P}(A) = \int_{-\infty}^{\infty} \mathbb{P}(A | Y = y) f_Y (y) \, \mathrm{d} y
    \end{equation}
    \noindent which is sort of like the continuous version of the law of total probability. 
\end{theorem}

\subsection{Properties of Conditional Expectation}

Conditional expectation has some useful properties:

\begin{itemize}
    \item If $X$ and $Y$ are \textit{independent}, then $ \mathbb{E(X | Y)} = \mathbb{E}(X)$
    \item For any function $h$, $ \mathbb{E}(h(Y) X|Y) = h(Y)\mathbb{E}(X|Y)$. 
    \item Linearity: $ \mathbb{E}(X_{1} + X_{2} | Y) = \mathbb{E}(X_{1} | Y) + \mathbb{E}(X_{2} | Y)$, and $ \mathbb{E}(cX | Y) = c \mathbb{E}(X | Y)$, $\forall c \in \mathbb{R}$. 
    \item Adam's Law: $ \mathbb{E}(\mathbb{E}(X | Y)) = \mathbb{E}(X)$. 
    \item Adam's Law with extra conditioning: For any r.v.s $X$, $Y$, $Z$, $\mathbb{E}(\mathbb{E}(X | Y, Z) | Z) = \mathbb{E}(X | Z)$. This is true because conditional probabilities are probabilities, so we are free to use Adam's Law here. 
\end{itemize}

\subsection{Conditional Variance}

\begin{definition}
    The \textbf{conditional variance} of $X$ given $Y = y$ is denoted by $\text{Var}(X | Y = y)$, and is just the variance of $X$ given that $Y$ takes the value $y$. A fundamental result about variance (Eve's Law) is: \begin{equation}
        \text{Var}(X) = \mathbb{E}(\text{Var} (X | Y)) + \text{Var}(\mathbb{E}(X | Y))
    \end{equation}
    \noindent which is also known as the \textbf{law of total variance}. 
\end{definition} 

\begin{proof}
    Let $g(Y) = \mathbb{E}(X | Y)$. By Adam's law, $ \mathbb{E}(g(Y)) = \mathbb{E}(Y)$. Then \begin{align*}
        \mathbb{E}(\text{Var}(X|Y)) &= \mathbb{E}(\mathbb{E}(X^{2}|Y) - g(Y)^{2}) = \mathbb{E}(X^{2}) - \textcolor{red}{\mathbb{E}(g(Y)^{2})} \\ 
        \text{Var}(\mathbb{E}(X | Y)) &= \mathbb{E}(g(Y)^{2}) - (\mathbb{E}(X))^{2} = \textcolor{red}{\mathbb{E}(g(Y)^{2})} - \mathbb{E}(X)^{2} 
    \end{align*}
    \noindent Now adding these 2 equations (removing the red terms), we have Eve's Law. 
\end{proof}

\section{Covariance and Correlation}

Covariance is a single-number summary of the Joint Distribution of two r.v.s, just like mean and variance for a single r.v. The covariance between r.v.s $X$ and $Y$ is: \begin{align*}
    \text{Cov} (X, Y) &= \mathbb{E} ((X - \mu_X)(Y - \mu_Y)) \\ 
    &= \mathbb{E}(XY) - \mathbb{E}(X)\mathbb{E}(Y)
\end{align*}

\noindent For general (not necessarily independent) r.v.s $X$ and $Y$, \begin{equation}
    \text{Var}(X+Y) = \text{Var}(X) + \text{Var}(Y) + 2 \text{Cov}(X,Y)
\end{equation}

\noindent If $X$ and $Y$ are \textit{independent}, then $\text{Cov}(X,Y) = 0$ (however, reverse implication is not true). \\

\noindent Intuitively, if $X$ and $Y$ tend to move in the \textbf{same direction}, then $X - \mu_X$ and $Y - \mu_Y$ will tend to be either both positive or negative, so $(X - \mu_X)(Y - \mu_Y)$ will be positive on average, so covariance is positive. If they move in \textbf{opposite directions}, then they tend to have opposite signs, giving a negative covariance. 

\begin{theorem}
    If $X$ and $Y$ are independent, then they are uncorrelated. \begin{proof}
        \begin{align*}
            \mathbb{E}(XY) &= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} xyf_X(x)f_Y(y) \, \mathrm{d}x \, \mathrm{d}y \\ 
            &= \int_{-\infty}^{\infty} yf_Y(y) \left( \int_{-\infty}^{\infty} xf_X(x) \, \mathrm{d}x\right) \, \mathrm{d}y \\ 
            &= \int_{-\infty}^{\infty} xf_X(x) \, \mathrm{d}x \int_{-\infty}^{\infty} yf_Y(y) \, \mathrm{d}y \\ 
            &= \mathbb{E}(X)\mathbb{E}(Y) \qedhere
        \end{align*}
    \end{proof}
\end{theorem}

\subsection{Properties of Covariance}

\begin{enumerate}
    \item $\text{Cov}(X,X) = \text{Var}(X)$
    \item $\text{Cov}(X,Y) = \text{Cov}(Y,X)$
    \item $\text{Cov}(X,c) = 0$ for any constant $c$
    \item $\text{Cov}(aX, Y) = a \text{Cov}(X,Y)$ for any constant $a$
    \item $\text{Cov}(X + Y, Z) = \text{Cov} (X,Z) + \text{Cov}(Y,Z)$
    \item $\text{Cov}(X+Y, Z+W) = \text{Cov}(X,Z) + \text{Cov}(X,W) + \text{Cov}(Y,Z) + \text{Cov}(Y,W)$
    \item $\text{Var}(X_{1} + X_{2} + \dots + X_n = \displaystyle\sum_{i=1}^{n} \displaystyle\sum_{j=1}^{n} \text{Cov}(X_i,X_j))$
    \item $\text{Var}(aX + bY) = a^{2}\text{Var}(X) + 2ab \text{Cov}(X,Y) + b^{2}\text{Var}(Y)$
\end{enumerate}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{Images/covariancegraph.png}
    \caption{Joint Distribution of $(X,Y)$ under various dependence structures.}
    \label{fig:10-covariancegraph}
\end{figure} 

\begin{theorem}
    For \textbf{independent r.v.s}, the variance of the sum is the sum of the variance: \begin{equation}
        \text{Var} \left( \sum_{i=1}^{n} X_i\right) = \sum_{i=1}^{n} \text{Var}(X_i)
    \end{equation}
\end{theorem}

\begin{theorem}
    If $X$ and $Y$ are independent, then the properties of covariance gives \begin{equation}
        \text{Var} (X-Y) = \text{Var}(X) + \text{Var} (Y)
    \end{equation}
\end{theorem}

\subsection{Correlation}

\begin{definition}
    The correlation between r.v.s $X$ and $Y$ is \begin{equation}\label{eq:10-correlation}
        \text{Corr} (X,Y) = \frac{\text{Cov}(X,Y)}{\sqrt{\text{Var}(X)}\text{Var}(Y)}
    \end{equation}
\end{definition}

\begin{theorem}
    Note that shifting and scaling $X$ and $Y$ has no effect on their correlation: \begin{equation*}
        \text{Corr}(cX,y) = \frac{\text{Corr}(cX,Y)}{\sqrt{\text{Var}(cX)}\text{Var}(Y)} = \frac{c \text{Cov}(X,Y)}{\sqrt{c^{2}\text{Var}(X) \text{Var}(Y)}} = \text{Corr}(X,Y)
    \end{equation*}
\end{theorem}

\begin{theorem}
    (Correlation Bounds) For any r.v.s $X$ and $Y$, \begin{equation*}
        -1 \leq \text{Corr}(X,Y) \leq 1
    \end{equation*}
\end{theorem}

\section{Bivariate Normal}

In order to fully specify a Bivariate Normal distribution for $(X,Y)$, we need to know five parameters: \begin{itemize}
    \item The means $\mathbb{E}(X)$, $\mathbb{E}(Y)$
    \item The variances $\text{Var}(X)$, $\text{Var}(Y)$
    \item The correlation $\text{Corr}(X,Y)$
\end{itemize}

\begin{definition}
    The r.v.s $X$ and $Y$ are said to have a \textbf{bivariate normal distribution} if their \textit{joint pdf} for all real $x$ and $y$ is given by: \begin{equation}
        f(x,y) = \frac{1}{2\pi \sigma_X \sigma_Y \sqrt{1-\rho^{2}}}\exp\left[-\frac{1}{2(1-\rho^{2})}Q(x,y)\right]
    \end{equation}
    \noindent where $\displaystyle Q(x,y) = \left( \frac{x - \mu_X}{\sigma_X}\right)^{2} - 2\rho \frac{x - \mu_X}{\sigma_X} \frac{y - \mu_Y}{\sigma_Y} + \left( \frac{y - \mu_Y}{\sigma_Y}\right)$
\end{definition}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{Images/bivariatenormal.png}
    \caption{Joint PDFs of two Bivariate Normal Distributions. On the left, $X$ and $Y$ are marginally $\mathcal{N}(0,1)$ and have 0 correlation. On the right, they have correlation 0.75.}
    \label{fig:10-bivariatenormal}
\end{figure} 

\begin{theorem}
    If each of $X$ and $Y$ is a linear combination of independent normal r.v.s $U_{1}, U_{2}, \dots, U_n$, then $X$ and $Y$  
\end{theorem}

\begin{theorem}
    If $X$ and $Y$ have a bivariate normal distribution, then the \textbf{marginal} pdf's $f_X$ and $f_Y$ are \textbf{also normal}, \begin{equation*}
        X \sim \mathcal{N}(\mu_X, \sigma_X^{2}), \quad Y \sim \mathcal{N}(\mu_Y, \sigma^{2}_Y), \quad \rho_{X,Y} = \rho
    \end{equation*} 
\end{theorem}


\begin{theorem}
    If $X$ and $Y$ have a bivariate normal distribution, then being \textbf{uncorrelated} ($\text{Cov}(X,Y) = \rho_{X,Y} = 0$) is the same as being \textbf{independent} - From equation \ref{eq:10-correlation}. 
\end{theorem}

\noindent In other words, for bivariate normal $X$ and $Y$, $\text{Cov}(X,Y) = 0$ if and only if $X$ and $Y$ are independent. 

\begin{theorem}
    (Independence of sum and difference) Let $X, Y \sim^{i.i.d} \mathcal{N}(0,1)$. The joint distribution of $(X + Y, X - Y)$: \begin{equation*}
        \text{Cov}(X + Y, X-Y) = \text{Var}(X) - \text{Cov}(X,Y) + \text{Cov}(Y,X) - \text{Var}(Y) = 0
    \end{equation*}
    \noindent $X + Y$ is independent of $X - Y$. Furthermore, they are i.i.d $\mathcal{N}(0,2)$. 
\end{theorem}

\begin{theorem}
    If $X$ an $Y$ have a bivariate normal distribution, then the conditional pdf of $X$ given $Y = y$ is also \textbf{normal} (vice versa). 
\end{theorem} The conditional pdf's can be visualised as cross-sections of the joint pdf. 

\begin{theorem}
    If $X$ and $Y$ have a bivariate normal distribution, then any \textit{linear combination} of $X$ and $Y$ is also \textbf{normal}. That is, for constants $a$ and $b$, if $W \sim aX + bY$, with $ \mathbb{E}(W) = \mu$ and $\text{Var}(W) = \sigma^{2}$, then $W \sim \mathcal{N}(\mu, \sigma^{2})$. 
\end{theorem}

\section{Poisson Processes}

\begin{definition}
    (1D Poisson Process) A sequence of arrivals in continuous time is a \textit{Poisson Process} with rate $\lambda$ if the following conditions hold: \begin{enumerate}
        \item The (average) number of arrivals in an interval of length $t$ is distributed with $\text{Poi}(\lambda t)$ (The rate is scalable with time, and the expected number of occurrences in any interval of length is $\lambda t$). 
        \item The numbers of arrivals in \textit{disjoint} time intervals are \textbf{independent}.  
    \end{enumerate}
\end{definition} A Poisson process describes the 'most random' way to distribute events in time. 

\begin{definition}
    Consider a Poisson process on $(0, \infty)$. Let $N(t)$ be the number of arrivals in $(0,t]$. It can be shown that $N(t) \sim \text{Poisson}(\lambda, t)$: \begin{equation}
        \mathbb{P}(N(t) = k) = e^{-\lambda t} \frac{(\lambda t)^{k}}{k!}
    \end{equation} Note that $N(t)$ only depends on the \textbf{length of the interval} (and not when the interval starts or ends). 
\end{definition}

\subsection{Interarrival Time}

\begin{definition}
    In a Poisson process, let $T_{1}$ be the time of the 1st arrival, $T_{2}$ be the time between the 1st and 2nd arrivals etc. The $T_i$'s are called interarrival times. \\ 
    \noindent In a Poisson process with rate $\lambda$, the interarrival times $T_i$'s are i.i.d. $\text{exponential}(\lambda)$ random variables. 
\end{definition} So the waiting time between successive events is $\text{exponential}(\lambda)$, while the \textit{arrival time} of the $n$th event is $\text{gamma}(n, \lambda)$. 

\subsection{Merging}
\begin{definition}
    Given 2 \textit{independent} Poisson processes, where 1 process has rate $\lambda_1$ and process 2 has rate $\lambda_2$, their \textbf{merge} is another Poisson process, with rate $(\lambda_1 + \lambda_2)$. 
\end{definition}


\section{Point Estimators}

A parameter $\theta$ is a constant but unknown value regarding a population. A \textbf{point estimate} $\hat{\theta}$ is a statistic computed from a sample and serves as a reasonable 'guess' for $\theta$. 

\subsection{Estimators as Random Variables}

\begin{definition} An estimator $\hat{\theta}$ is also a random variable. \begin{enumerate}
        \item The \textbf{bias} of an estimator $\hat{\theta}$ is defined to be $ \mathbb{E}(\hat{\theta}) - \theta$. If the bias is identically 0, then $\hat{\theta}$ is \textbf{unbiased} (i.e. if $ \mathbb{E}(\hat{\theta}) = \theta$) 
        \item The \textbf{variance} of an estimator $\hat{\theta}$ is $ \text{Var} (\hat{\theta}) = \mathbb{E} (\hat{\theta}^{2}) - \mathbb{E}(\hat{\theta})^{2}$. 
        \item The \textbf{mean square error} of an estimator $\hat{\theta}$ is defined to be $ \mathbb{E}((\hat{\theta} - \theta^{2}))$ (as small as possible). 
    \end{enumerate}
\end{definition}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{Images/biasvariancetradeoff.png}
    \caption{Bias Variance Tradeoff Visualisation}
    \label{fig:13-biasvariancetradeoff}
\end{figure} 

\noindent It can also be shown that \begin{equation*}
    \text{MSE}(\hat{\theta}) = \text{Var}(\hat{\theta}) + \text{bias}(\hat{\theta})^{2}
\end{equation*} hence, it would make sense to find the estimator that minimises MSE (but it is too difficult.) 

\noindent A common approach is to insist that we use \textit{unbiased estimators}, and if there are \textit{multiple unbiased estimators}, we pick the one with the \textbf{minimum variance} (MVUE). 

\subsection{Maximum Likelihood Estimation}

MLE does not always produce unbiased estimators, but it has the following properties: \begin{itemize}
    \item As $n \to \infty$, MLE converges to $\theta$, becomes asymptotically unbiased, and also asymptotically minimises the variance. 
    \item Given a function $g$, if the MLE for $\theta$ is $\hat{\theta}$, then the MLE for $g(\theta)$ is just $g(\hat{\theta})$. 
\end{itemize}


\subsection{Method of Moments}

If sample size $n$ is large, then the sample mean and the sample variance should be close to the true mean and the true variance respectively. The steps are: \begin{enumerate}
    \item If a distribution has one parameter to estimate, we equare the sample mean with the tru mean of the distribution (solve the equation thereafter). 
    \item If a distribution has \textbf{two parameters}, then we equate the sample mean with the true mean, and sample variance with true variance (solve two simultaneous equations). 
    \item The solutions are known as the moment estimators.
\end{enumerate}

\section{Introductory Bayesian Statistics}

\textit{Frequentists} interpret a probability as the limiting frequency of an event, as the event gets repeated multiple times. \begin{equation*}
    \mathbb{P}(E) = \lim_{n \to \times} \frac{n_E}{n}
\end{equation*} However, many real life events cannot be repeated, and they require \textit{Bayesian} methods to be analysed. \\

\noindent Bayesian definitions of probability $ \mathbb{P}(E)$ reflects our prior beliefs, so $ \mathbb{P}(E)$ can be any probability distribution, provided that it is consistent with all our beliefs. 

\subsection{Prior}
Let $\theta$ denote a parameter to be estimated. In the Bayesian approach, $\theta$ is considered to be \textbf{random} and the goal is to identify $\theta$ as data becomes available (typically we have some prior belief about $\theta$ from past data). This information can be incorporated into a probability distribution for $\theta$, known as the \textbf{prior distribution} $g(\theta)$. 

\subsection{Posterior}
Bayes' theorem says that $ \mathbb{P}(\theta | \text{data})= \mathbb{P}(\text{data} | \theta) \mathbb{P}(\theta) / \mathbb{P}(\text{data})$, i.e. \begin{equation}\label{eq: 14-posterior}
    \color{red} h(\theta | x_{1}, \dots , x_n) = \color{Aquamarine} \frac{f(x_{1}, \dots, x_n | \theta)\color{LimeGreen}g(\theta)}{\color{Apricot} f(x_{1}, \dots, x_n)}
\end{equation}

\begin{enumerate}
    \item $h(\theta | x_{1}, \dots , x_n)$ is known as the \textbf{posterior distribution} of $\theta$. All inference about $\theta$ is based on $h$. 
    \item Denominator obtained from the law of total probability (which is just a normalising constant, in practice it is rarely computed). 
\end{enumerate}

\begin{definition}
    \textcolor{red}{posterior} = \textcolor{Apricot}{constant} $\times$ likelihood function $\times$ prior
\end{definition}

\subsection{Beta Distribution}

\begin{definition}
    A continuous random variable $X$ is said to follow a \textbf{beta distribution} with parameters $a > 0$ and $b > 0$, if its pdf is given by \begin{equation*}
        f(x) = \begin{cases}
            \frac{\Gamma (a + b)}{\Gamma (a) \Gamma (b)} x^{a-1} (1-x)^{b-1}, \quad & \text{if } 0 \leq x \leq 1 \\ 
            0, \quad & \text{otherwise}
        \end{cases}
    \end{equation*} and this is denoted by $X \sim \text{beta} (a,b)$. 
\end{definition}

\noindent If $X \sim \text{beta}(a,b)$, then $ \mathbb{E}(X) = \frac{a}{a+b}$. The \textbf{posterior mean} is an example of a \textbf{Bayes estimator} for $\theta$. \\ 

\noindent Observe that the \textit{uniform prior} is a special case of a beta distribution with $a = 1, \; b = 1$. 

\subsection{General Result: Binomial}

\begin{theorem}
    In general, if we use a beta($a$, $b$) prior distribution for $\theta = \mathbb{P}(\mathcal{T})$, and observe $k$ $\mathcal{T}$'s out of $n$ tosses, then the posterior distribution of $\theta$ is beta($k+a$, $n-k+b$), and the posterior mean is \begin{equation*}
        \hat{\theta} = \frac{k + a }{n + a + b}
    \end{equation*}
\end{theorem}

\noindent Note that the posterior mean always lies \textit{between} the prior mean $\frac{a}{a+b}$, and the sample mean $\frac{k}{n}$ from the data. 

\subsection{Conjugate Prior}
When the data is Binomial (or Bernoulli), then a beta prior leads to a beta posterior. 

\subsubsection{Normal Distribution}

We will find the posterior when the data and the prior are both normal. Using the normal pdf and equation \ref{eq: 14-posterior}, we get \begin{align*}
    h(\theta | x_{1}, \dots , x_n) &= C f(X_{1}, \dots, x_n | \theta)g(\theta) \\ 
    &= C_{1} \exp \left[ - \frac{(x_{1}-\theta)^{2}}{2\sigma^{2}} - \dots - \frac{(x_n - \theta)^{2}}{2\sigma^{2}} - \frac{(\theta - \mu_0)^{2}}{2\sigma_0^{2}} \right]
\end{align*}

\noindent We 

\section{Prediction Interval}

In statistical inference, specifically predictive inference, a prediction interval is an estimate of an interval in which a future observation will fall, with a certain probability, given what has already been observed. Prediction intervals are often used in regression analysis. \\ 

\noindent We model a random sample as i.i.d random variables $X_{1}, X_{2}, \dots , X_n$, where each $X_i \sim \mathcal{N}(\mu, \sigma^{2})$. Since any linear combination of independent normals is also normal, we see that $\bar{X} \sim \mathcal{N}(\mu, \sigma^{2})$. We consider a future observation $X_{n+1}$, where $X_{n+1} \sim \mathcal{N}(\mu, \sigma^{2})$, then $N_{n+1} - \bar{X}$ is also normal, and we have \begin{equation*}
    X_{n+1} - \bar{X} \sim \mathcal{N}(0, \sigma^{2}(1 + 1 / n))
\end{equation*} where we have relied on the \textbf{normality assumption} and not $n$ being large. Then, we have the prediction random variable: \begin{equation*}
    T_{n-1} \sim \frac{X_{n+1} - \bar{X}}{S \sqrt{1+ 1 / n}}
\end{equation*} We convert this into an interval: \begin{theorem}
    A $100(1-\alpha)\%$ \textbf{prediction interval} for a future observation drawn from a normal distribution is given by \begin{equation*}
        \left[ \bar{x} - t_{n-1, \alpha / 2}s_x \sqrt{1 + 1 / n}, \; \bar{x} + t_{n-1, \alpha / 2}s_x \sqrt{1 + 1 / n}  \right]
    \end{equation*}
\end{theorem} In general, a prediction interval is much wider than a confidence interval, as we are not only uncertain about the values of $\mu$ and $\sigma^{2}$, we are also trying to predict the value pf a random variable. In particular, the width does not go to $0$ as $n \to  \infty$. 

\section{Hypothesis Testing}
The first step of hypothesis testing is defining the parameters of interest (e.g. $\mu$). We then set up two hypotheses to represent two claims: a null hypothesis $H_{0}$, and an alternative hypothesis $H_{1}$. \begin{enumerate}
    \item $H_{0}$ is a conservative stance: no difference or change. We write this as $\theta = \theta_0$.
    \item $H_{1}$ is a radical stance that \textbf{contradicts} $H_{0}$: there is a change to be made. We write this as $\theta \neq \theta_0$. 
\end{enumerate} The logic involving testing hypotheses is the same as proof by contradiction: assume $H_{0}$ to be true, then perform calculations to determine whether the data contradicts this assumption. \begin{enumerate}
    \item If Yes, we reject $H_{0}$ in favour of $H_{1}$. 
    \item If No, we do not reject $H_{0}$ (data does not provide enough evidence for us to believe in $H_{1}$). 
\end{enumerate}

\subsection{Error Types}

\begin{table}[H]
    \centering
    \begin{tabular}{| c | c | c | c |}
        \hline & & & True State of Nature \\ \hline
        & & $H_{0}$ is true & $H_{1}$ is true \\ \hline  
        Our decision based on data & Reject $H_{0}$ & Type I Error (FP) & Correct Decision \\ \hline 
        & Not Reject $H_{0}$ & Correct Decision & Type II Error (FN) \\ \hline 
    \end{tabular}
    \caption{Types of Errors}
    \label{16-typeoferror}
\end{table}

\subsection{Testing for Variance}
\begin{enumerate}
    \item For $H_{0}$: $\sigma^{2} = \sigma^{2}_0$ vs $H_{1}$: $\sigma^{2} \neq \sigma^{2}_0$, reject $H_{0}$ if $\sigma^{2}$ is outside the CI \begin{equation*}
        \left[ \frac{(n-1)s^{2}_x}{\chi^{2}_{n-1, \; \alpha / 2}}, \;  \frac{(n-1)s^{2}_x}{\chi^{2}_{n-1, \; 1-\alpha / 2}}\right]
    \end{equation*}
    \item For $H_{0}$: $\sigma^{2} > \sigma^{2}_0$ vs $H_{1}$: $\sigma^{2} \neq \sigma^{2}_0$, reject $H_{0}$ if $\sigma^{2}$ is outside the CI \begin{equation*}
        \left[ \frac{(n-1)s^{2}_x}{\chi^{2}_{n-1, \; \alpha / 2}}, \;  \infty\right)
    \end{equation*}
    \item For $H_{0}$: $\sigma^{2} < \sigma^{2}_0$ vs $H_{1}$: $\sigma^{2} \neq \sigma^{2}_0$, reject $H_{0}$ if $\sigma^{2}$ is outside the CI \begin{equation*}
        \left( 0, \;  \frac{(n-1)s^{2}_x}{\chi^{2}_{n-1, \; 1-\alpha / 2}}\right]
    \end{equation*}
\end{enumerate}



\newpage 

\section*{References}

Some references used in these notes: \\ 

Introduction to Probability, Joe Blitzstein \& Jessica Hwang. 

\end{document}