\documentclass{article}
\usepackage{LectureNotes}

\setstretch{1.2}

\begin{comment}
\geometry{
    textheight=9in,
    textwidth=5.5in,
    top=1in,
    headheight=12pt,
    headsep=25pt,
    footskip=30pt
}
\end{comment}


\geometry
{
    a4paper,
    total={170mm,257mm},
    left=20mm,
    top=20mm,
}


% ------------------------------------------------------------------------------

\begin{document}

% ------------------------------------------------------------------------------
% Cover Page and ToC
% ------------------------------------------------------------------------------

\title{ \normalsize \textsc{}
		\\ [2.0cm]
		\HRule{1.5pt} \\
		\LARGE \textbf{\uppercase{40.002 Optimization}
		\HRule{2.0pt} \\ [0.6cm] \LARGE{An Introduction to Optimization} \vspace*{10\baselineskip}}
		}
\date{\today}
\author{\textbf{Michael Hoon}}

\maketitle
\newpage

\tableofcontents
\newpage

% ------------------------------------------------------------------------------

\section{Introduction to Linear Programming}
An optimization problem is defined by:

\begin{itemize}
    \item \textbf{Decision variables}: elements under the control of the decision maker
    \item \textbf{A (single) objective function}: a function of the decision variables that we want to optimize, corresponding to a criterion for measuring maximize
    \item \textbf{Constraints}: restrictions that define which values of the decision variables are allowed.
\end{itemize}

\noindent We want to find the \textbf{minimum} or \textbf{maximum} of a function of one or many variables subject to a set of \textbf{constraints}:

\begin{align}
    & \min \; f(x_{1}, \dots x_n) \\ \nonumber
    & \ni(x_{1}, \dots x_n) \in \chi \subseteq \mathbb{R}^{n}
\end{align}

\noindent where the decision variables are vectors $x_{1}, \dots x_n$, the objective function is $f(x_{1}, \dots x_n)$ and the constraints are defined by the set $\chi \subseteq \mathbb{R}^{n}$. A vector $\mathbf{x}^{*}$ is called \textit{optimal}, or a \textit{solution} of the problem, if it has the \textbf{smallest objective value} among all vectors that satisfy the constraints. 

\subsection{Linear Programming}
A linear program is a class of optimisation problem in which the objective and all constraint functions are \textbf{linear}, given by:

\begin{align}
    & \min \; \mathbf{c}^{\top} \mathbf{x} \\ \nonumber
    & \ni \mathbf{Ax} \geq \mathbf{b}, \; \text{and} \; \mathbf{x} \geq 0
\end{align}

\noindent where the decision vector is $\mathbf{x}$ (n variables), linear objective function: $f(\mathbf{x}) = \mathbf{c}^{\top} \mathbf{x} = c_{1}x_{1} + \dots + c_n x_n$, and the linear constraints are $\chi = \{\mathbf{x} \in \mathbb{R}^{n}\mid \mathbf{Ax} \geq \mathbf{b}\}$ (m constraints) \footnote[1]{Note: vector inequalities are interpreted componentwise.}. Note that matrix $\mathbf{A}_{(m\times n)}$ is of $m \times n$ dimension. 

\subsubsection{Inequality Transformations}

We have matrix $\mathbf{A}$ given by:

\begin{equation*}
    \mathbf{A} = \begin{pmatrix}
        - & \mathbf{a_1}^{\top} & - \\ 
        \vdots & \vdots & \vdots \\ 
        - & \mathbf{a_m}^{\top} & -
    \end{pmatrix}
\end{equation*}

\begin{itemize}
    \item An equality constraint $\mathbf{a_i}^{\top} \mathbf{x} = b_i$ is equivalent to two equality constraints $\mathbf{a_i}^{\top} \mathbf{x} \leq b_i$ and $\mathbf{a_i}^{\top} \mathbf{x} \geq b_i$
    \item An inequality constraint $\mathbf{a_i}^{\top} \mathbf{x} \leq b_i$ is equivalent to the inequality constraint $-\mathbf{a_i}^{\top} \mathbf{x} \geq -b_i$ (Note the negatives applied to both sides of the inequality).
    \item Constraints such as $x_j \geq 0, \; x_j \leq 0$ can be expressed in the form $\mathbf{a_i}^{\top} \mathbf{x} \geq b_i$ by appropriately choosing $\mathbf{a}_i, \; b_i$.
\end{itemize}

\noindent Note that there is no simple analytic formula for the solution of a linear program, but there are a variety of effective methods for solving them, including Dantzig's simplex method, and the more recent interior-point methods. We cannot give the exact number of arithmetic operations required to solve a linear program, but we can establish rigorous bounds on the number of operations required to solve a linear program using an interior-point method (in practice, this is of the order $n^{2}m$, assuming $m \geq n$). 

\subsubsection{Terminology}

\begin{definition}
    We now introduce some terminology for geometric linear programming:
    \begin{itemize}
        \item A \textbf{linear function} $f : \mathbb{R}^{n} \to \mathbb{R}$ is a function of the form: \begin{equation*}
            f(x_{1}, \dots , x_n) = a_{1}x_{1} + \dots + a_n x_n, \quad a_n \in \mathbb{R}
        \end{equation*}
        \item A \textbf{hyperplane} in $\mathbb{R}^{n}$ is the set of points satisfying a single linear equation:
        \begin{equation*}
            a_{1}x_{1} + \dots + a_n x_n = b, \quad a_n \in \mathbb{R}
        \end{equation*}
        \item A \textbf{halfspace} in $\mathbb{R}^{n}$ is the set of points satisfying a single linear inequality: 
        \begin{equation*}
            a_{1}x_{1} + \dots + a_n x_n \geq b, \quad a_n, b \in \mathbb{R}
        \end{equation*}
        \item A \textbf{polyhedron} is a set that can be described by a finite number of halfspaces. A \textbf{polytope} is a \textbf{bounded} polyhedron.
        \item An assignment of values to the decision variables is a \textbf{feasible solution} if it satisfies all the constraints (infeasible otherwise). The set of all feasible solutions is the \textbf{feasible region.}
        \item An \textbf{optimal solution} is a feasible solution that achieves the \textbf{best possible objective function value}. For a minimisation problem, $x^{*}$ is optimal \textbf{iff} $\mathbf{c}^{\top} \mathbf{x^{\top}} \leq \mathbf{c}^{\top} \mathbf{x}$ for all feasible $\mathbf{x}$. 
        \item We call $\mathbf{c}^{\top} x^{*}$ the \textbf{optimal objective value}. 
        \item $\forall K \in \mathbb{R}$ we can find a feasible solution $\mathbf{x}$ such that $\mathbf{c}^{\top}\mathbf{x} \leq K$, then the linear program in \textbf{minimisation} form has \textbf{unbounded} cost. The optimum cost is then $-\infty$. In this case, we can find a feasible $\mathbf{x}$ and direction $\mathbf{d}$ such that $\mathbf{x} + t \mathbf{d}$ is feasible $\forall t \geq 0$ and $\mathbf{c}^{\top}d < 0$.
    \end{itemize}    
\end{definition}

\subsection{Geometric Definition}

In a simple two-dimensional space with the equation $x_{1} + x_{2} = z$, this function can be represented by a line. The decision variables are $x_{1}$ ad $x_{2}$, and this line represents all possible combinations of $x_{1}$ and $x_{2}$ that yield the same objective value $z$. \\ 

\noindent Each constraint is a linear inequality, which creates a boundary in the solution space. The feasible region is now the polygon formed by the intersection of all these constraint boundaries. \\

\begin{tikzpicture}[>=stealth, scale=1.5]
    % Objective function
    \draw[->] (-0.5,0) -- (3,0) node[below]{$x_1$};
    \draw[->] (0,-0.5) -- (0,3) node[left]{$x_2$};
    \draw[dashed, domain=0:2.5, variable=\x, blue] plot ({\x}, {2-\x}) node[right]{$c^Tx = z$};
  
    % Constraints
    \filldraw[fill=gray!20] (0,0) -- (2,0) -- (1.2,1.8) -- cycle;
    \draw[thick] (2,0) -- (0,0) node[below left]{$O$};
    \draw[thick] (1.2,1.8) -- (1.2,0) node[below]{$x_1$};
    \draw[thick] (1.2,1.8) -- (0,1.8) node[left]{$x_2$};
    
    % Labels
    \node[below] at (2,0) {$b_1$};
    \node[left] at (0,1.8) {$b_2$};
  
    % Optimal solution
    \filldraw[red] (1.2,1.2) circle (1.5pt) node[above right]{$(x^*,y^*)$};
    
    % Arrow to optimal solution
    \draw[->, red] (1.5,1.5) -- (1.25,1.25);
  
    % Feasible Region label
    \node[gray!70, align=center] at (1,0.9) {Feasible\\Region};
  \end{tikzpicture}

\noindent The feasible region is often convex, meaning that if points A and B are inside the region, the line segment connecting A and B are also inside the region. Some examples are shown below: \\

\begin{tikzpicture}[>=stealth, scale=1.5]
    % Convex Feasible Region
    \filldraw[fill=gray!20] (0,0) -- (2,0) -- (1.5,1.5) -- (0.5,2) -- cycle;
  
    % Convex Examples
    \filldraw[fill=blue!30, xshift=4cm] (0,0) -- (1.5,0) -- (1,1.5) -- cycle;
    \filldraw[fill=green!30, xshift=8cm] (0,0) -- (1,0.5) -- (1.5,1.5) -- (0.5,1.5) -- cycle;
  
    % Labels
    \node at (0.75,1) {Convex Feasible Region};
    \node at (4.75,1) {Example 1};
    \node at (8.75,1) {Example 2};
  
    % Convex Hulls
    \draw[dashed, red] (0,0) -- (1.5,1.5) -- (0.5,2) -- (0,0);
    \draw[dashed, red, xshift=4cm] (0,0) -- (1,1.5) -- (1.5,0) -- (0,0);
    \draw[dashed, red, xshift=8cm] (0,0) -- (1.5,1.5) -- (0.5,1.5) -- (0,0);
  \end{tikzpicture} \\

\subsection{Feasible Regions}
\begin{theorem}
    The feasible region of a linear program is \textbf{convex}. 
\end{theorem}

\noindent Mathematically, a set $\chi \subseteq \mathbb{R}^{n}$ is convex if 

\begin{equation*}
    \lambda \mathbf{x} + (1-\lambda)\mathbf{y} \in \chi, \; \forall \mathbf{x}, \mathbf{y} \in \chi, \; \forall \lambda \in [0,1]
\end{equation*}



\section{Simplex Method}

\section{Duality}

\section{Sensitivity Analysis}

\section{Application to Game Theory}

\section{Robust Optimization}

\section{Maximum Matching}

\section{Network Simplex Algorithm}

\section{Integer Programming}

\section{LP Relaxation}

\section{Branch-and-Bound}

\section{Cutting Planes}

\section{Dynamic Programming}

\section{Travelling Salesman Problem}

\end{document}